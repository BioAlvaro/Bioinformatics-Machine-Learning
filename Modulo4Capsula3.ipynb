{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Modulo4Capsula3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4E-m14iq0P4"
      },
      "source": [
        "![cabecera_slide_abiertaugr_bigdata.jpg](https://i.imgur.com/HXn24wC.jpg)\n",
        "## **Módulo 4 - Aprendizaje Supervisado: Técnicas de Regresión.**\n",
        "## **4.3 Métodos de *Machine Learning* para regresión.**\n",
        "* ### $k$-vecinos para regresión\n",
        "* ### Modelo de regresión $M5$ (*Cubist*)\n",
        "\n",
        "**Autores**: \n",
        "\n",
        "*Por* María José Gacto\n",
        "\n",
        "Profesora contratada doctor de la Universidad de Jaén.\n",
        "\n",
        "Departamento de Informática, Universidad de Jaén, Jaén, 23071, España.\n",
        "\n",
        "*Y* Augusto Anguita-Ruiz\n",
        "\n",
        "Becario i-PFIS (ISCIII) en Universidad de Granada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Y_zJaSHFjR"
      },
      "source": [
        "**Recordatorio: Introducción a NoteBook**\n",
        "\n",
        "Dentro de este cuaderno (*NoteBook*), se le guiará paso a paso desde la carga de un conjunto de datos hasta el análisis descriptivo de su contenido.\n",
        "\n",
        "El cuaderno de *Jupyter* (Python) es un enfoque que combina bloques de texto (como éste) junto con bloques o celdas de código. La gran ventaja de este tipo de celdas, es su interactividad, ya que pueden ser ejecutadas para comprobar los resultados directamente sobre las mismas. *Muy importante*: el orden las instrucciones es fundamental, por lo que cada celda de este cuaderno debe ser ejecutada secuencialmente. En caso de omitir alguna, puede que el programa lance un error, así que se deberá comenzar desde el principio en caso de duda.\n",
        "\n",
        "Antes de nada:\n",
        "\n",
        "Es muy muy importante que al comienzo se seleccione \"*Abrir en modo de ensayo*\" (draft mode), arriba a la izquierda. En caso contrario, no permitirá ejecutar ningún bloque de código, por cuestiones de seguridad. Cuando se ejecute el primero de los bloques, aparecerá el siguiente mensaje: \"*Advertencia: Este cuaderno no lo ha creado Google.*\". No se preocupe, deberá confiar en el contenido del cuaderno (*NoteBook*) y pulsar en \"Ejecutar de todos modos\".\n",
        "\n",
        "¡Ánimo!\n",
        "\n",
        "Haga clic en el botón \"play\" en la parte izquierda de cada celda de código. Las líneas que comienzan con un hashtag (#) son comentarios y no afectan a la ejecución del programa.\n",
        "\n",
        "También puede pinchar sobre cada celda y hacer \"*ctrl+enter*\" (*cmd+enter* en Mac).\n",
        "\n",
        "Cada vez que ejecute un bloque, verá la salida justo debajo del mismo. La información suele ser siempre la relativa a la última instrucción, junto con todos los `print()` (orden para imprimir) que haya en el código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UeDnwimIWSN"
      },
      "source": [
        "## **ÍNDICE** \n",
        "\n",
        "En este *notebook*: \n",
        "1. Aprenderemos los conceptos generales de la técnica de k-vecinos más cercanos para regresión.\n",
        "2. Aprenderemos los conceptos generales de la técnica de aprendizaje de modelos de regresión M5.\n",
        "3. Aplicaremos ambas técnicas como alternativas más automatizadas frente a la regresión sobre nuestro conjunto de datos de obesidad infantil.\n",
        "    \n",
        "Contenidos:\n",
        "1. [KNN ($k$-vecinos más cercanos para regresión)](#etiquetaA)   \n",
        "2. [Modelo de regresión M5](#etiquetaB)   \n",
        "3. [Instalación de R, bibliotecas y lectura de los datos de obesidad infantil](#etiquetaC)   \n",
        "4. [Aplicación de KNN al problema](#etiquetaD)   \n",
        "5. [Aplicación de M5 al problema](#etiquetaE)   \n",
        "6. [Validación cruzada para ambas técnicas: Comparación y elección de los modelos](#etiquetaF)   \n",
        "7. [Bibliografía](#etiquetaG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosumELxpCqc"
      },
      "source": [
        "## **1. KNN ($K$-VECINOS MÁS CERCANOS PARA REGRESIÓN) <a id='etiquetaA'></a>**\n",
        "\n",
        "> *Idea básica: «Si grazna como un pato, camina como un pato y se comporta como un pato, entonces, ¡Seguramente es un pato!»*\n",
        "\n",
        "Se basa en las mismas ideas que su conocida versión para clasificación. Necesita de los siguientes tres componentes: Disponer del conjunto de datos de entrenamiento con la posibilidad de poder almacenarlo, una métrica para calcular la distancia entre cualquier par de datos, y un valor de $k$ (número de vecinos cercanos a considerar durante la estimación). Una vez se dispone de los tres componentes mencionados, ***no hay algoritmo de aprendizaje ni ajuste de un modelo***, la idea principal del **algoritmo del vecino más cercano ($k=1$)** es:\n",
        "\n",
        "* Almacenar todas las instancias/datos de entrenamiento $<x^i,f(x^i)>$\n",
        "*\tDada una instancia nueva de consulta $x^q$, primero encontrar la instancia de entrenamiento más cercana $x^n$, y entonces estimar directamente que $f(x^q) = f(x^n)$\n",
        "\n",
        "Resulta bastante evidente el hecho de que para un **algoritmo de k-vecinos más cercanos ($k=1$)** siempre se obtendrá error cero con el propio conjunto de entrenamiento. Aunque sea algo que puede resultar impresionante para quienes aún no tengan mucha experiencia, hay que recordar que el objetivo final de cualquier algoritmo de aprendizaje automático es el de generalizar correctamente, es decir, el de inferir con precisión cuando se presenten nuevos casos que no han sido vistos previamente. El hecho de que esta técnica se ajuste de manera tan extrema a los datos de entrenamiento suele provocar un alto sobreaprendizaje y, por lo tanto, una capacidad de generalización bastante pobre con errores muy elevados en los datos de *test*.\n",
        "\n",
        "Es por ello, que salvo en situaciones muy específicas, se suele evitar el uso de $k=1$. En el caso de un **algoritmo de k-vecinos más cercanos ($k>1$)**, dada una instancia nueva de consulta $x^q$, reemplazaríamos la estimación directa por:\n",
        "\n",
        "   *\tClasificación (variable categórica) -  Escoger el voto entre sus $k$ vecinos más cercanos como estimación de $f(x^q)$. Se suelen utilizar valores impares de $k$ como una forma de intentar evitar empates en el voto.\n",
        "   *\t**Regresión (variable continua)** - Tomar la **media** de los valores $f$ de sus $k$ vecinos más cercanos como estimación, $f(x^q) = \\sum_{i=1}^k f(x^i) / k$. En este caso es indiferente tener valores de $k$ pares o impares (véase la Figura 1 para un ejemplo con $k=3$, donde las líneas horizontales provienen de la media de los tres ejemplos más cercanos).\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=12jVUjKy1jWHofZNQNpv2ZyAB8xZhv4Jf' />\n",
        "<figcaption>Figura 1. Ejemplo de ajuste con un 3-vacinos más cercanos</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpnRkJQDECkh"
      },
      "source": [
        "### **1.1 Medida de distancia**\n",
        "\n",
        "**La medida de distancia más comunmente utilizada es la distancia euclídea o de Minkowski**, aunque no sea la única. También se utilizan otras métricas de distancia como la distancia absoluta, la de *Manhattan*. Los algoritmos suelen implementar la distancia de Minkowski, ya que permite variar la métrica de distancia simplemente cambiando el valor de un parámetro:\n",
        "\n",
        "<center> $d(x^i, x^j) = \\left( \\sum_{v=1}^p \\vert x_v^i - x_v^j \\vert^q \\right)^{\\frac{1}{q}}$, q=1 (absoluta o Manhattan); q=2 (euclídea); etc. </center>\n",
        "\n",
        "En cualquier caso **es necesario normalizar previamente los datos** para que todas las variables afecten equitativamente (aunque en algunas implementaciones ya se hace internamente). Para entender la importancia de la normalización proponemos el siguiente ejemplo: las horas medias de trabajo al día de una persona tienen que estar dentro del rango de las 24 horas (y en teoría no deberían ser superiores a 8), mientras que su salario podría variar de los miles a los cientos de miles. Si no se normalizan los valores de las variables, pequeños cambios en el salario (50.000 a 51.000 euros/año) pesarían muchísimo más que grandes cambios en el número de horas (4 a 8 horas). En concreto, para esos valores tendríamos $\\vert x_{salary}^i - x_{salary}^j \\vert^2 = 1.000.000$ *versus* $\\vert x_{hours}^i - x_{hours}^j \\vert^2 = 16$, con lo que las horas de trabajo prácticamente no influirían en el cáculo de las distancias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlB4dVgGD8ty"
      },
      "source": [
        "### **1.2 Escogiendo el valor de $k$**\n",
        "\n",
        "Debemos tener en cuenta que **la elección del parámetro $k$ puede afectar negativamente en las estimaciones**. Podemos guiarnos por las siguientes dos premisas:\n",
        "\n",
        "* Si $k$ es demasiado pequeño, sensible a los puntos de ruido.\n",
        "* Si $k$ es demasiado grande, el vecindario puede incluir puntos no representativos.\n",
        "\n",
        "Una manera de mitigar estos problemas es el calculo de una media ponderada por la distancia a los $k$ puntos. El caso más extremo de KNN ponderado por distancias, utiliza directamente todo los datos del conjunto de entrenamiento y es conocido como método de Shepard. También existe lo que se denomina funciones Kernel. Una función Kernel sirve para regular la manera en que la distancia a los $k$ puntos afecta al cálculo de la media ponderada para obtener la salida final. Algunos ejemplos son: El kernel rectangular, kernel triangular, kernel de Epanechnikov, kernel de Gauss, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_ahd5kNm2k"
      },
      "source": [
        "## **2. MODELO DE REGRESIÓN M5 <a id='etiquetaB'></a>**\n",
        "\n",
        "Como ya se ha indicado en la cápsula 1 de este módulo, **M5 podría ser considerado casi un paradigma en el área de la regresión** (véase el estudio realizado en 2019 entre 164 algorimos por Gacto et. al. donde un simple árbol se muestra competitivo contra los mejores *ensembles* de 500 árboles - *Sección de Bibliografía*). El modelo de regresión M5 (*model tree*) se basa en una estructura de tipo árbol de regresión (arboles de decisión con constantes numéricas en sus hojas en lugar de clases). La principal diferencia de los modelos de regresión respecto a dichos árboles de regresión, es que éstos incluyen funciones lineales multi-variables en las hojas en lugar de una constante. En definitiva, **lo que tenemos es un conjunto de funciones lineales por trozos**, haciéndolo perfecto para ajustarse a todo tipo de datos tanto lineales como no lineales. **Véase en la Figura un ejemplo de modelo obtenido con M5, que incluye 7 funciones lineales distintas a aplicar según se cumplan las condiciones aprendidas para las distintas variables contempladas en el árbol**. A diferencia de los métodos de regresión que hemos visto hasta ahora, M5 **si que permite** y se aprovecha de variables de tipo categórico no ordinal. Así, en el ejemplo mostrado en la figura tenemos que: $Si\\ \\  pgain<=3.5\\ \\ y\\ \\ motor=B\\ ó\\ A$ se aplicaría el modelo lineal LM3 (aprovechando la variable categórica $motor$ como variable de decisión en un nodo); y que sin embargo $Si\\ \\  pgain \\in (3.5,4.5]\\ \\ y\\ \\ vgain>2.5$ se aplicaría el modelo lineal LM5 (atendiendo sólo a variables numéricas en este caso).\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1mXF7vc735rFsZ6kD8cjAIfPN59x0Fiz4' />\n",
        "<figcaption>Figura 2. Ejemplo de modelo obtenido con M5</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "Cuando está correctamente implementado, M5 es uno de los métodos más eficientes, y **puede manejar conjuntos de datos con una dimensionalidad relativamente alta (hasta cientos de atributos, siendo aún así un método bastante rápido)**. Esta capacidad diferencia a M5 de otros tipos de técnicas de regresión como por ejemplo MARS (*Multivariate Adaptive Regression Splines*), cuyo coste computacional (tiempo de ejecución) crece muy rápidamente cuando el número de variables de entrada aumenta. Por otro lado, **una ventaja de M5 sobre CART y otros árboles de regresión es que los modelos obtenidos son generalmente mucho más pequeños/simples y demuestran ser más precisos**.\n",
        "\n",
        "---\n",
        "\n",
        "> **NOTA:** *A partir de aquí se presenta el funcionamiento del método de aprendizaje M5, es decir, la manera a través de la que, a partir de los datos de entrenamiento para un problema dado, se obtienen modelos de regresión como el del ejemplo mostrado en la figura. Entendemos, que si bien las técnicas de regresión descritas hasta el momento podrían haber representado cierto nivel de esfuerzo en cuanto a su comprensión, no deberían de haber sido muy difíciles de seguir y entender. Sin embargo, en este punto, debido a la mayor complejidad técnica de M5, el contenido se hará más técnico y dependiendo de los conocimientos previos puede llegar a ser más difícil de entender.*\n",
        "\n",
        "> *Como el hecho de cursar este MOOC demuestra un interés por iniciarse en el área de la Bioinformática (y a pesar de que en este caso se requiere de una explicación mucho más técnica y \"quizás\" aburrida), se ha tomado la decisión de incluir a continuación una descripción lo más breve y simplificada posible del algoritmo M5 para quienes tengan la curiosidad sana de saber como funcionan estos modelos. Saber cómo se aprenden los modelos hará mucho más fácil la comprensión posterior de los mismos. Por todo ello, hemos creido conveniente incluir una muy breve explicación del método para quién quiera seguirla. No obstante, **si con lo explicado hasta ahora ha entendido a lo que nos referimos cuando hablamos de un modelo de regresión obtenido por M5** (un árbol con condiciones desde su raíz hasta las hojas, las cuales determinan la aplicación local de pequeños modelos lineales) **podría continuar directamente con las explicaciones de la cápsula a partir de la siguiente sección (Sección 3)**, donde veremos como ejecutar KNN y M5 sobre nuestro conjunto de datos de obesidad infantil (y saltarse esta explicación).*\n",
        "\n",
        "---\n",
        "\n",
        "El método M5 fue propuesto por **J.R. Quinlan en 1992**, y aunque es uno de los métodos clásicos, a día de hoy sigue reconociéndose como uno de los más potentes. A pesar de que M5 emplea el mismo enfoque que el árbol de regresión *CART* al intentar minimizar el Error Cuadrático Medio (*ECM*) con su función de impureza o de ganancia (utilizada para decidir sobre las posibles divisiones de un nodo), como se ha mencionado, M5 no asigna una constante al nodo de la hoja sino que en su lugar se ajusta a un modelo de regresión lineal multi-variable. En lo que sigue veremos los tres componentes principales del algoritmo de aprendizaje que permiten describir su funcionamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0sgpVWD1Ru"
      },
      "source": [
        "### **2.1 Construcción del árbol**\n",
        "\n",
        "El método M5 sigue la **estrategia de división recursiva de nodos** de los árboles de decisión. Supongamos que en un paso dado del algoritmo (un nodo del arbol a generar) disponemos de un grupo de instancias o datos de entrenamiento $T$. O bien $T$ se asociará con una hoja, o bien se escogerá alguna de las posibles divisiones atendiendo al criterio de impureza o ganancia, dividiendo $T$ en dos subgrupos. El mismo proceso se aplica recursivamente a todos los subconjuntos. Este proceso a menudo produce estructuras con sobre-aprendizaje que posteriormente deben ser podadas.\n",
        "\n",
        "Para conseguir **minimizar el *ECM*** cuando se intenta aplicar una división, la ganancia de información que dicha división provocaría se mide por la reducción de la desviación estándar en los valores de la variable de salida de los datos, antes y después de la división. Para ello, el primer paso es calcular la desviación estándar (*standard deviation -sd-*, como se la conoce en Inglés) de los valores de salida de las instancias en $T$, $sd(T)$. En el caso de que $T$ contenga muy pocas instancias (por ejemplo menos de 4) o que $sd(T)$ ya sea menor que un pequeño porcentaje de la $sd$ inicial, el nodo se convertiría directamente en nodo hoja. En otro caso, se calcularán todas las posibles divisiones de $T$ para cada posible punto de corte sobre cada variable de entrada. Llamemos $T_i$ al subgrupo de instancias $i-th$ correspondiente a una división específica. Si la desviación $sd(T_i)$ de los valores de la variable de salida de las instancias en $T_i$ se utiliza como una medida del error, la reducción esperada del error puede ser definida de la siguiente manera:\n",
        " \n",
        "<center> $\\triangle error = sd(T) - \\sum_i  \\frac{|T_i|}{|T|} sd(T_i)$ </center>\n",
        "\n",
        "**La división que maximice la reducción de errores esperada será la que finalmente se aplique**. Aunque esta formulación trata de ser lo más general posible, en el caso de M5 (como en la mayoría de algoritmos basados en árboles), el número máximo de subconjuntos que cada división generará es dos. Es decir, una división sólo daría lugar a los subgrupos $T_1$ y $T_2$. Por lo tanto, para cada nodo con división sólo se generan dos nodos hijo (árbol binario)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nit_iZ1DuRq"
      },
      "source": [
        "### **2.2 Poda del árbol**\n",
        "\n",
        "La poda se realiza desde las hojas hasta el nodo raíz. En cada nodo interno, M5 **compara el error estimado de ese nodo** (como si no se hubiese dividido y fuese un nodo hoja) **y el error estimado del sub-árbol que de él cuelga**. Entonces, **el sub-árbol se poda si no produce una mejora** suficiente en el rendimiento del árbol.\n",
        "\n",
        "El factor clave de este método es cómo estima ese error para que no se sobre-aprenda demasiado y se generalice bien en nuevas instancias no vistas. M5 lo calcula promediando primero la diferencia absoluta entre los valores de la variable de salida de los datos de entrenamiento y los valores de sus correspondientes predicciones. Esto generalmente subestimará el error en instancias no vistas, por lo que M5 lo multiplica por $(n + v)=(n - v)$, donde $n$ es el número de instacias asociadas al nodo y $v$ es el número de parámetros en el modelo lineal que quedaría si finalmente fuese una hoja. La intención es aumentar el error estimado de modelos con muchos parámetros construidos a partir de un pequeño número de instancias. El error estimado de un subárbol se calcula como la suma ponderada del error estimado de sus ramas izquierda y derecha, multiplicando por la proporción de muestras que descienden al árbol izquierdo y derecho, respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZfvtdBLDcg2"
      },
      "source": [
        "### **2.3 Obtención de los modelos lineales**\n",
        "\n",
        "Un modelo lineal multi-variable se ajusta a los datos de entrenamiento en cada nodo del árbol utilizando técnicas de regresión estándar. Antes de realizar la poda, aprovechando la vuelta hacia atrás de la recursividad utilizada para generar el arbol, **se ajusta un modelo de regresión lineal para cada nodo sobre las instancias de entrenamiento asociadas al mismo**.\n",
        "\n",
        "Comienza, por lo tanto, por los **nodos hoja** considerando solo las **variables utilizadas en las divisiones desde la raíz del árbol hasta el nodo hoja** (se mira hacia arriba). Sin embargo, en el caso de los **nodos internos** (aquellos que no son hoja/terminales), M5 no utiliza las variables de las divisiones mirando hacia arriba, sino que se restringe a las **variables que son referenciadas por las divisiones o por los modelos lineales en los subárboles que cuelgan de dicho nodo** (se mira hacia abajo). Estos modelos lineales internos se utilizan únicamente para poder aplicar los criterios de poda. Como M5 comparará la precisión de dicho modelo lineal con la precisión de los subárboles que cuelgan de dicho nodo, esto garantiza unas condiciones de competencia equitativas en las que los dos tipos de modelos utilizan la misma información.\n",
        "\n",
        "Además, después de construir cualquier modelo lineal M5 lo simplifica, eliminando los coeficientes uno a uno mediante un algoritmo voraz (al estilo del enfoque descendente que utilizamos en la cápsula 2 de este módulo pero automatizado). Por lo general, esto podría resultar en un aumento en el error promedio, sin embargo también reducen los factores multiplicativos anteriores, por lo que el error estimado (para las nuevas instancias aún no vistas) puede disminuir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m39pteJyR2G-"
      },
      "source": [
        "## **3. INSTALACIÓN DE R, BIBLIOTECAS Y LECTURA DE LOS DATOS DE OBESIDAD INFANTIL <a id='etiquetaC'></a>**\n",
        "\n",
        "Como se explicó en la primera cápsula de este módulo, necesitamos ejecutar las siguientes 3 celdas antes de empezar con los algoritmos Knn y M5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU5gcUFGSpZl"
      },
      "source": [
        "# Tiempo estimado de ejecución: 20 segundos aprox.\n",
        "\n",
        "### Instalación de R en notebooks de Google Colab ###\n",
        "!apt-get update\n",
        "!apt-get install r-base\n",
        "!pip install rpy2\n",
        "%load_ext rpy2.ipython\n",
        "print (\"Instalación de R en Google Colab terminada\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJX4puBixZ0L"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2:15 minutos aprox.\n",
        "# Bibliotecas necesarias:\n",
        "# ISLR para regresión lineal multivariable\n",
        "# kknn para k-vecinos más cercanos de regresión\n",
        "# Cubist para modelos de regresión basados en M5\n",
        "\n",
        "%%R\n",
        "### Instalación de las bibliotecas necesarias\n",
        "#install.packages(c(\"ISLR\", \"kknn\", \"Cubist\"))\n",
        "install.packages(c(\"kknn\", \"Cubist\")) #ISLR se utilizaba en la anterior cápsula\n",
        "print (\"Instalación de las bibliotecas de R para este módulo terminada\")\n",
        "\n",
        "### Importación de las bibliotecas necesarias ###\n",
        "#require(ISLR)\n",
        "require(kknn)\n",
        "require(Cubist)\n",
        "print (\"Importación de las bibliotecas de R para este módulo terminada\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSwe0RnTJOx"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Lectura\n",
        "data <- read.csv(url(\"https://drive.google.com/uc?id=1GO2NBxYw54K6HkN-YgXbNadrLo5O6-0u\"))\n",
        "\n",
        "### Visualización de una pequeña parte de los datos\n",
        "head(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0mgPydswday"
      },
      "source": [
        "## **4. APLICACIÓN DE KNN AL PROBLEMA <a id='etiquetaD'></a>**\n",
        "\n",
        "Una vez importadas las librerías y leídos los datos estamos en disposición de aplicar $k$-vecinos más cercanos a nuestro problema ($k$-nn). La implementación de $k$-nn que vamos a utilizar está incluida como parte de la librería *kknn* de R, que ya hemos instalado en nuestro entorno. La función también es conocida como $kknn(.)$ y sigue la siguiente sintaxis:\n",
        "\n",
        "<center> $kknn (formula = formula(train), train, test, k = 7, distance = 2, kernel = \"optimal\", scale=TRUE)$</center>\n",
        "\n",
        "\t(Aplica k-NN a un conjunto de test dado un conjunto de entrenamiento -> Recordemos!: No hay algoritmo de aprendizaje)\n",
        "\n",
        "*formula* – variables usadas al estilo de lm; *train y test* – conjuntos de datos de entrenamiento y test; *k* – número de vecinos (si no se indica es 7); *distance* – distancia de Minkowski, es decir con 1 Manhattan y con 2 euclídea (si no se indica es 2); *scale* – escalado de las variables para tener igual desviación estandar (si no se indica es TRUE); *kernel* – tipo de kernel usado para el uso de pesos según distancias (si no se indica es \"optimal\"). El \"rectangular\" es $k$-NN estandar sin pesos. Tipos existentes: \"rectangular\", \"triangular\", \"epanechnikov\", \"biweight\", \"tri-weight\", \"cos\", \"inv\", \"gaussian\", \"rank\" y \"optimal“.\n",
        "\n",
        "Nosotros dejaremos los parámetros por defecto y veremos como ejecutarlo, calcular errores, etc., mediante el uso del conjunto completo como entrenamiento y test por ahora. De esta manera veremos también cómo le afectan algunas variables variando la fórmula e incluso probando con la misma fórmula que obtuvimos para la regresión lineal en la cápsula anterior. También mostraremos cómo aplicar una validación cruzada más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1R12xi-xhTk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "4cf41674-3701-45ba-d02a-77444720b9e7"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "\n",
        "%%R\n",
        "\n",
        "fitKNN <- kknn(HOMA ~ ., data, data)\n",
        "yprime = fitKNN$fitted.values\n",
        "cat('\\n(fórmula con todas las variables) RMSE:', sqrt(sum((data$HOMA-yprime)^2)/length(yprime)), \"\\n\") #RECM->en inglés RMSE\n",
        "\n",
        "fitKNN_lm <- kknn(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+I(BMI^2), data, data)\n",
        "yprime = fitKNN_lm$fitted.values\n",
        "cat('(fórmula obtenida con lm) RMSE:', sqrt(sum((data$HOMA-yprime)^2)/length(yprime)), \"\\n\") #RECM->en inglés RMSE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(fórmula con todas las variables) RMSE: 0.4974704 \n",
            "(fórmula obtenida con lm) RMSE: 0.5062813 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlFtrTt2BMM"
      },
      "source": [
        "Según los valores obtenidos, está claro que **la fórmula obtenida para otra técnica no tiene por qué comportarse de igual manera con $k$-vecinos más cercanos**. En el caso de la regresión lineal se consiguieron mejoras significativas mientras que ahora los resultados incluso empeoran.\n",
        "\n",
        "Este comportamiento es el esperado. Esta técnica se basa en distancias y normalmente si se consiguen mejoras es porque se eliminan algunas variables que puedan entorpecer o contrarrestar las distancias calculadas para el resto. En lo que sigue, se puede comprobar el resultado final de intentar eliminar algunas variables. Al probar a quitar todas las variables de una en una, la que mayor reducción del error proporciona es \"Sex\". En una segunda iteración también se puede quitar la variable de presión sistólica \"SBP\". Y a partir de ahí, seguir quitando variables ya sólo produce un empeoramiento del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIyAE4EP5fMS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aca9469f-a747-4655-d55f-4d86a7ebd7f6"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "\n",
        "%%R\n",
        "\n",
        "fitKNN <- kknn(HOMA ~ .-Sex-SBP, data, data)\n",
        "yprime = fitKNN$fitted.values\n",
        "cat('\\n(fórmula con todas las variables) RMSE:', sqrt(sum((data$HOMA-yprime)^2)/length(yprime)), \"\\n\") #RECM->en inglés RMSE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(fórmula con todas las variables) RMSE: 0.4824407 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0e6ZUPq8rbe"
      },
      "source": [
        "Igualmente, también se puede probar a incluir algún término no lineal que mejore el cálculo de las distancias. En nuestro caso sabíamos que $BMI$ mostraba un comportamiento cuadrático. Vamos a probar a incluirlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbMmpUUE8oDG"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "\n",
        "%%R\n",
        "\n",
        "fitKNN <- kknn(HOMA ~ .-Sex-SBP+I(BMI^2), data, data)\n",
        "yprime = fitKNN$fitted.values\n",
        "cat('\\n(fórmula con todas las variables) RMSE:', sqrt(sum((data$HOMA-yprime)^2)/length(yprime)), \"\\n\") #RECM->en inglés RMSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T5x6Kvs7Yy9"
      },
      "source": [
        "Visualización:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYApQE2m9h8k"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "#yprime = fitKNN$fitted.values\n",
        "\n",
        "%%R\n",
        "plot(data$HOMA~data$BMI)\n",
        "points(data$BMI,yprime,col=\"red\",pch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCrUqwoUAr5U"
      },
      "source": [
        "**¡¡¡ ¿Y no se nos ha olvidado algo? !!!**\n",
        "\n",
        "**¡¡¡ ¿Qué pasa con la NORMALIZACIÓN de los datos? !!!**\n",
        "\n",
        "Efectivamente, es un paso crucial que no debemos olvidar. Si no lo hubiésemos tenido en cuenta, tendríamos que desechar todo lo realizado, normalizar los datos y repetir todo el proceso. Pero... ¿no lo hemos tenido en cuenta? No es nuestro caso, el algoritmo utilizado ya realiza internamente dicha normalización. Si recordamos los parámetros que aceptaba la función $kknn(.)$ como entrada, teníamos $scale=TRUE$, que hace que por defecto la normalización esté activa. Éste parámetro se utiliza para decirle si queremos o no que normalice los datos antes de aplicar el algoritmo (TRUE o FALSE).\n",
        "\n",
        "No todas las implementaciones de $k$-vecinos más cercanos permiten realizar la normalización como parte del propio algoritmo. Por ello, es esencial prestar mucha atención a éste aspecto y realizar la normalización a mano si la versión que utilicemos no los normaliza por sí sola."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKm1xVtxRkk"
      },
      "source": [
        "## **5. APLICACIÓN DE M5 AL PROBLEMA <a id='etiquetaE'></a>**\n",
        "\n",
        "En esta sección veremos cómo aplicar M5 a nuestro problema de estimación de la insulino-resistencia. La implementación de M5 que vamos a utilizar está incluida como parte de la librería *Cubist* de R, que ya hemos instalado en nuestro entorno. En realidad, Cubist es una extensión de M5 que incluye una serie de iteraciones de M5 basadas en *boosting* para combinar varios árboles. La combinación de árboles o *ensembles* representa técnicas más complejas o avanzados que no son objeto de este MOOC de introducción a la bioinformática. Además, tampoco está del todo claro que Cubist funcione significativamente mejor que su algoritmo base, y definitivamente no es mejor que un *bagging* de M5 (técnica alternativa al *boosting* para igualmente combinar varios árboles). Véase el estudio de *Gacto et. al.* 2019 donde se ve que M5 se muestra competitivo con respecto a Cubist y un *bagging* de M5 es la técnica con mejor puntuación - *Sección de Bibliografía*.\n",
        "\n",
        "Nos centraremos, por lo tanto, en M5 indicándole mediante parámetro a la función $cubist(.)$ que el número de iteraciones del *boosting* es igual a 1. De esta manera, tenemos justamente el comportamiento del M5 original. La función es conocida como $cubist(.)$ y sigue la siguiente sintaxis:\n",
        "\n",
        "<center> $cubist(x, y, committees = 1)$</center>\n",
        "\n",
        "\t(Aplica Cubist a los datos de entrenamiento con X's e Y separados, y obtiene un único árbol para el consenso entre modelos -> M5)\n",
        "\n",
        "*x* – una matriz de datos con los valores de las variables de entrada.  Se permiten valores perdidos pero únicamente numéricos. También se permiten valores categóricos; *y* – un vector numérico con los correspondientes valores de la variable de salida; *committees* – un entero, número de modelos para el consenso (si no se indica es 1 -> M5).\n",
        "\n",
        "Al igual que con $k$-nn veremos como ejecutarlo, calcular errores, y en este caso visualizar el árbol, mediante el uso del conjunto completo separando X's e Y. No obstante, M5 ya selecciona las variables más relevantes durante el propio proceso de aprendizaje, por lo que en este caso no tendremos que hacerlo manualmente. De hecho, podemos ver cómo la función $cubist(.)$ no incluye ninguna manera de especificar la *fórmula*. Igualmente, mostraremos cómo aplicar una validación cruzada más adelante junto con $k$-nn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiW1Ji9S-eRa"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "\n",
        "%%R\n",
        "\n",
        "tam <- length(names(data))\n",
        "fitM5 <- cubist(x = data[, -tam], y = data$HOMA) #committees = 1\n",
        "yprime = predict(fitM5,data)\n",
        "cat('\\n(M5) RMSE:', sqrt(sum((data$HOMA-yprime)^2)/length(yprime)), \"\\n\") #RECM->en inglés RMSE\n",
        "\n",
        "summary(fitM5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNoVJL9J_2Xk"
      },
      "source": [
        "En este caso, se puede ver cómo se obtiene un modelo con RECM de $0.5392671$ que incluye distintas líneas (o modelos lineales) atendiendo a distintos puntos de corte en $BMI$ y $Age$. Desde que empezamos con el problema de insulino-resistencia hemos visto que estas dos variables son probablemente las más determinantes. Tiene mucho sentido que M5 las haya seleccionado para tomar las decisiones que conforman el árbol."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw5xpxq39yDj"
      },
      "source": [
        "Visualización:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvO5qHcv93ud"
      },
      "source": [
        "#yprime = predict(fitM5,data)\n",
        "\n",
        "%%R\n",
        "plot(data$HOMA~data$BMI)\n",
        "points(data$BMI,yprime,col=\"green\",pch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHyRa7rpxR0h"
      },
      "source": [
        "## **6. VALIDACIÓN CRUZADA PARA AMBAS TÉCNICAS: COMPARACIÓN Y ELECCIÓN DE LOS MODELOS <a id='etiquetaF'></a>**\n",
        "\n",
        "En este punto, ya hemos llegado a ejecutar todos los métodos estudiados. Hemos obtenido distintos valores de error, *RECM*. Sin embargo, no podemos apresurarnos para escoger un posible método ganador. Como bien sabemos, necesitamos realizar una estimación del error real para nuestro problema realizando una validación cruzada, y comparando por los errores medios obtenidos en los conjuntos de *test*. Ya vimos cómo calcularlo para la regresión lineal multi-variable, con la que obtuvimos un *RECM* de $0.6956061$. En esta última sección mostramos como aplicar validación cruzada con $k$-nn y M5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExHtbcnz08gJ"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "### KNN ###\n",
        "\n",
        "%%R\n",
        "set.seed(123456)  #fijamos la misma semilla utilizada con regresión lineal\n",
        "                  #para conseguir las mismas particiones\n",
        "k <- 5            \n",
        "data$kfold <- sample(1:k, nrow(data), replace = T)\n",
        "\n",
        "performances <- c()\n",
        "# Una iteración por fold\n",
        "for (fold in 1:k){\n",
        "  # Se crea el conjunto de entrenamiento para la iteración\n",
        "  training_set <- data[data$kfold != fold,]\n",
        "  nombres <- names(training_set)\n",
        "  tam <- length(nombres)-1\n",
        "  training_set <- training_set[,nombres[1: tam]]\n",
        "    \n",
        "  # Se crea el conjunto de test para la iteración\n",
        "  testing_set <- data[data$kfold == fold,]\n",
        "  nombres <- names(testing_set)\n",
        "  tam <- length(nombres)-1\n",
        "  testing_set <- testing_set[,nombres[1: tam]]\n",
        "\n",
        "  ## Entrenando el modelo para la iteración\n",
        "  model <- kknn(HOMA ~ .-Sex-SBP+I(BMI^2), training_set, testing_set)\n",
        "\n",
        "  ## Calculando el error de test\n",
        "  yprime = model$fitted.values  #yprime <- predict(model, testing_set)\n",
        "                                #cambiamos porque no funciona predict con kknn,\n",
        "                                #ya que el propio kknn es el predict\n",
        "  RMSE <- sqrt(sum((testing_set$HOMA-yprime)^2)/length(yprime))\n",
        "\n",
        "  # Se añade el RECM a la lista de errores\n",
        "  performances[fold] <- RMSE\n",
        "}\n",
        "\n",
        "#Eliminamos la columna artificial añadida para kfold\n",
        "#(para que no acumule columnas si se ejecuta varias veces)\n",
        "nombres <- names(data)\n",
        "tam <- length(nombres)-1\n",
        "data <- data[,nombres[1: tam]]\n",
        "\n",
        "cat(\"RECM medio en test para 5-fcv en K-nn:\", mean(performances))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwTBkGnE0g4f"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "### M5 ###\n",
        "\n",
        "%%R\n",
        "set.seed(123456)  #fijamos la misma semilla utilizada con regresión lineal\n",
        "                  #para conseguir las mismas particiones\n",
        "k <- 5            \n",
        "data$kfold <- sample(1:k, nrow(data), replace = T)\n",
        "\n",
        "performances <- c()\n",
        "# Una iteración por fold\n",
        "for (fold in 1:k){\n",
        "  # Se crea el conjunto de entrenamiento para la iteración\n",
        "  training_set <- data[data$kfold != fold,]\n",
        "  nombres <- names(training_set)\n",
        "  tam <- length(nombres)-1\n",
        "  training_set <- training_set[,nombres[1: tam]]\n",
        "    \n",
        "  # Se crea el conjunto de test para la iteración\n",
        "  testing_set <- data[data$kfold == fold,]\n",
        "  nombres <- names(testing_set)\n",
        "  tam <- length(nombres)-1\n",
        "  testing_set <- testing_set[,nombres[1: tam]]\n",
        "\n",
        "  ## Entrenando el modelo para la iteración\n",
        "  tam <- length(names(training_set))\n",
        "  model <- cubist(x = training_set[, -tam], y = training_set$HOMA) #committees = 1\n",
        "\n",
        "  ## Calculando el error de test\n",
        "  yprime <- predict(model, testing_set)\n",
        "  RMSE <- sqrt(sum((testing_set$HOMA-yprime)^2)/length(yprime))\n",
        "\n",
        "  # Se añade el RECM a la lista de errores\n",
        "  performances[fold] <- RMSE\n",
        "}\n",
        "\n",
        "#Eliminamos la columna artificial añadida para kfold\n",
        "#(para que no acumule columnas si se ejecuta varias veces)\n",
        "nombres <- names(data)\n",
        "tam <- length(nombres)-1\n",
        "data <- data[,nombres[1: tam]]\n",
        "\n",
        "cat(\"RECM medio en test para 5-fcv en M5:\", mean(performances))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWt4LP2LzGvz"
      },
      "source": [
        "A continuación se muestran los resultados que obtuvimos cuando estuvimos seleccionando las variables para fijar la fórmula o viendo cómo se ejecuta M5, es decir, sin validación cruzada:\n",
        "* Regresión lineal sin validación - RECM: 0.6507716\n",
        "* $k$-nn sin validación - RECM: **$0.4590836$**\n",
        "* M5 sin validación - RECM: 0.5392671\n",
        "\n",
        "En este caso hubiésemos escogido $k$-nn como técnica para aplicar a nuestro problema (si pensamos sólo en precisión y no en interpretación). \n",
        "\n",
        "Sin embargo comprobemos qué resulta cuando realizamos la validación cruzada:\n",
        "* Regresión lineal con validación - RECM: **$0.6956061$**\n",
        "* $k$-nn con validación - RECM: 0.7929245\n",
        "* M5 con validación - RECM: 0.7807851\n",
        "\n",
        "Saque sus propias conclusiones. De esta manera, el paso final cuando se ejecutan distintas técnicas para resolver un problema dado sería comparar los errores medios en test obtenidos por validación cruzada, seleccionar la mejor (o las mejores) atendiendo a dichos valores y/o a criterios de interpretabilidad, y una vez fijada la técnica volver a aplicarla con todos los datos para obtener un modelo final candidato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAsoqNeHo2nF"
      },
      "source": [
        "## **REFERENCIAS BIBLIOGRÁFICAS <a id='etiquetaE'></a>**\n",
        "\n",
        "**KNN**\n",
        "- Hechenbichler K. and Schliep K.P. (2004) Weighted k-Nearest-Neighbor Techniques and Ordinal Classification, Discussion Paper 399, SFB 386, Ludwig-Maximilians University Munich (http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper399.ps)\n",
        "\n",
        "- Samworth, R.J. (2012) Optimal weighted nearest neighbour classifiers. Annals of Statistics, 40, 2733-2763. (http://www.statslab.cam.ac.uk/~rjs57/Research.html)\n",
        "\n",
        "**M5**\n",
        "-\tGareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n",
        "An Introduction to Statistical Learning with Applications in R\n",
        "Springer, 2013 (**Chapter 08**)\n",
        "-\tJ. R. Quinlan, Learning With Continuous Classes, In Proc. of the 5th Australian Joint Conf. on Artificial Intelligence, pp. 343-348, 1992\n",
        "- Y. Wang and I.H. Witten, Inducing Model Trees for Continuous Classes, In Proc. of the 9th European Conf. on Machine Learning, pp. 128-137, 1997\n",
        "\n",
        "**Librerías**\n",
        "- Usando rpy2 en notebooks: https://rpy2.github.io/doc/latest/html/notebooks.html\n",
        "- Usando read.csv de R: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table\n",
        "- Usando kknn de R: https://cran.r-project.org/web/packages/kknn/index.html\n",
        "- Usando Cubist de R: https://cran.r-project.org/web/packages/Cubist/index.html\n",
        "\n",
        "## **REFERENCIAS ADICIONALES**\n",
        "\n",
        "-\tM.J. Gacto, J.M. Soto-Hidalgo, J. Alcalá-Fdez, and R. Alcalá (2019). Experimental Study on 164 Algorithms Available in Software Tools for Solving Standard Non-Linear Regression Problems. IEEE Access 7, 2019, pp. 108916-108939; https://doi.org/10.1109/ACCESS.2019.2933261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_y70_DKpCqq"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "MOOC Machine Learning y Big Data para la Bioinformática (1ª edición)   \n",
        "\n",
        "http://abierta.ugr.es     \n",
        "</div>    "
      ]
    }
  ]
}