{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Modulo4Capsula2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4E-m14iq0P4"
      },
      "source": [
        "![cabecera_slide_abiertaugr_bigdata.jpg](https://i.imgur.com/HXn24wC.jpg)\n",
        "## **Módulo 4 - Aprendizaje Supervisado: Técnicas de Regresión.**\n",
        "## **4.2 Métodos clásicos de regresión.**\n",
        "* ### La regresión Lineal Simple y Multiple\n",
        "\n",
        "**Autores**: \n",
        "\n",
        "*Por* Rafael Alcalá\n",
        "\n",
        "Catedrático de la Universidad de Granada\n",
        "\n",
        "Instituto Andaluz Interuniversitario en Data Science and Computational Intelligence (DaSCI)\n",
        "\n",
        "*Y* Augusto Anguita-Ruiz\n",
        "\n",
        "Becario i-PFIS (ISCIII) en Universidad de Granada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Y_zJaSHFjR"
      },
      "source": [
        "**Recordatorio: Introducción a NoteBook**\n",
        "\n",
        "Dentro de este cuaderno (*NoteBook*), se le guiará paso a paso desde la carga de un conjunto de datos hasta el análisis descriptivo de su contenido.\n",
        "\n",
        "El cuaderno de *Jupyter* (Python) es un enfoque que combina bloques de texto (como éste) junto con bloques o celdas de código. La gran ventaja de este tipo de celdas, es su interactividad, ya que pueden ser ejecutadas para comprobar los resultados directamente sobre las mismas. *Muy importante*: el orden las instrucciones es fundamental, por lo que cada celda de este cuaderno debe ser ejecutada secuencialmente. En caso de omitir alguna, puede que el programa lance un error, así que se deberá comenzar desde el principio en caso de duda.\n",
        "\n",
        "Antes de nada:\n",
        "\n",
        "Es muy muy importante que al comienzo se seleccione \"*Abrir en modo de ensayo*\" (draft mode), arriba a la izquierda. En caso contrario, no permitirá ejecutar ningún bloque de código, por cuestiones de seguridad. Cuando se ejecute el primero de los bloques, aparecerá el siguiente mensaje: \"*Advertencia: Este cuaderno no lo ha creado Google.*\". No se preocupe, deberá confiar en el contenido del cuaderno (*NoteBook*) y pulsar en \"Ejecutar de todos modos\".\n",
        "\n",
        "¡Ánimo!\n",
        "\n",
        "Haga clic en el botón \"play\" en la parte izquierda de cada celda de código. Las líneas que comienzan con un hashtag (#) son comentarios y no afectan a la ejecución del programa.\n",
        "\n",
        "También puede pinchar sobre cada celda y hacer \"*ctrl+enter*\" (*cmd+enter* en Mac).\n",
        "\n",
        "Cada vez que ejecute un bloque, verá la salida justo debajo del mismo. La información suele ser siempre la relativa a la última instrucción, junto con todos los `print()` (orden para imprimir) que haya en el código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny9tgvVapCqc"
      },
      "source": [
        "## **ÍNDICE** \n",
        "\n",
        "En este *notebook*: \n",
        "1. Aprenderemos los conceptos generales de la técnica de regresión Lineal Simple y Multiple.\n",
        "2. Aplicaremos la regresión Lineal Multiple como primera herramienta de estudio sobre cualquier problema de regresión real (con ejemplos sobre nuestro problema de estimación de la insulino-resistencia).\n",
        "    \n",
        "Contenidos:\n",
        "1. [Regresión lineal](#etiquetaA)   \n",
        "2. [Instalación de R, bibliotecas y lectura de los datos de obesidad infantil](#etiquetaB)   \n",
        "3. [Primera toma de contacto con el problema y estudio de las variables de mayor interés](#etiquetaC)  \n",
        "4. [Selección aditiva de variables: Enfoque descendente](#etiquetaD)   \n",
        "5. [Interacciones y no linealidad](#etiquetaE)   \n",
        "6. [Validación cruzada](#etiquetaF)   \n",
        "7. [Bibliografía](#etiquetaG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aosumELxpCqc"
      },
      "source": [
        "## **1. REGRESIÓN LINEAL <a id='etiquetaA'></a>**\n",
        "\n",
        "El primer **método de regresión** que abordaremos en este módulo es la **regresión lineal**, considerada como un enfoque simple dentro del aprendizaje supervisado. En la **regresión lineal**, se asume que la **dependencia de la variable de salida $Y$** (en nuestro caso de estudio sobre obesidad infantil representada por la variable *HOMA-IR*) **sobre las variables de entrada $X_1, X_2, \\dots, X_p$** (resto de variables de nuestro problema) **es lineal**. Aunque pueda parecer demasiado simplista, la **regresión lineal** es extremadamente útil tanto conceptualmente como en la práctica. En esta cápsula veremos cómo hacer uso de la misma para estudiar un conjunto de datos y poder sacar conclusiones últiles sobre el comportamiento de los mismos, sin perjuicio de que luego se utilicen otras técnicas (supuestamente o en teoría más potentes) para obtener modelos mejor ajustados. En un primer lugar, haremos una distinción entre **regresión lineal simple o múltiple**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgPxq_nzF7co"
      },
      "source": [
        "### **1.1 Conceptos básicos sobre la regresión lineal simple**\n",
        "\n",
        "En una **regresión lineal simple**, usando **una sola variable de entrada $X$** asumimos el siguiente modelo,\n",
        "\n",
        "<center>$Y = \\beta_0 + \\beta_1 X + \\epsilon,$ </center>\n",
        " \n",
        "donde $\\beta_0$ y $\\beta_1$ son dos constantes desconocidas que representan el término independiente y la pendiente de una función lineal (una recta) respectivamente. A estas constantes se las conoce como **coeficientes o parámetros**. Por otro lado, $\\epsilon$ hace referencia al **término de error de la estimación**.\n",
        "\n",
        "Dada una estimación $\\hat{\\beta_0}$ y $\\hat{\\beta_1}$ para los **coeficientes** del modelo, podemos predecir valores futuros de la variable de salida $Y$ utilizando,\n",
        " \n",
        "<center>$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x,$ </center>\n",
        "\n",
        "donde $\\hat{y}$ representa una predicción de $Y$ sobre la base de $X = x$. El símbolo del sombrero denota que nos estamos refiriendo a **valores estimados** (en lugar de a valores reales u observados). Los valores de $\\hat{\\beta_0}$ y $\\hat{\\beta_1}$ se obtienen por la **técnica matemática de mínimos cuadrados** que obtiene los coeficientes que minimizan el error cometido para cada instancia disponible en un conjunto de datos de entrenamiento.  Dicho de otra forma, por **mínimos cuadrados** se obtienen siempre los valores óptimos de los **coeficientes** que minimizan el valor de **RECM**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNSg6c3OGF7b"
      },
      "source": [
        "### **1.2 Conceptos básicos sobre la regresión lineal múltiple**\n",
        "\n",
        "En el caso de la **regresión lineal múltiple**, usando **más de una variable de entrada $X$** asumimos el siguiente modelo,\n",
        "\n",
        "<center>$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon,$ </center>\n",
        "\n",
        "interpretando cada coeficiente $\\beta_j$ como el efecto promedio en $Y$ de una unidad de incremento en $X_j$, manteniendo el resto de variables de entrada fijas. El caso ideal es cuando las variables de entrada $X's$ no están correlados entre sí (no existe colinealidad), ya que esto implicaría ciertos problemas de interpretación. No obstante, si el proceso de aprendizaje se realiza paso a paso, atendiendo a los valores estadísticos obtenidos sobre dichos coeficientes, las variables que están correladas entre sí terminan por ser eliminadas. En la **regresión lineal múltiple**, la estimación de los coeficientes también se realiza mediante la **técnica de mínimos cuadrados**, minimizando el error de las predicciones sobre el conjunto de datos de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPbDyUB7GNR2"
      },
      "source": [
        "### **1.3 Evaluación de la bondad de la estimación de los coeficientes**\n",
        "\n",
        "Esta técnica genera ciertos **valores estadísticos** que nos permitirán contestar a preguntas del tipo; *¿Existe al menos una variable de entrada ($X_j$) que tenga relación lineal con la variable de salida ($Y$)?*, *En caso afirmativo, ¿Cuál/es es/son?*. Los **valores estadísticos** de los que hablamos son dos:\n",
        "\n",
        "*\t**Estadístico F**: El estadístico F es un valor que se obtiene para el modelo de regresión de manera completa, en lugar de para cada una de las variables de entrada. En la medida en la que su valor se aleje de 1 (tanto hacia arriba como hacia abajo) indica que al menos hay una variable de entrada que presenta relación lineal con la variable de salida. Si éste valor es cercano a 1 podemos directamente descartar la regresión lineal modelada, ya que no habrá ninguna variable de entrada ($X_j$) con relación lineal con la salida ($Y$). Este parámetro resulta de especial importancia en problemas con un elevado número de variables de entrada, entre las que puede haber variables que en un principio se muestren como relevantes sin realmente serlo. Por este motivo, el **estadístico F** es el primer parámetro que debemos de estudiar al realizar una regresión. Tras ello, **solo se continuará con el proceso** si su valor no está cercano a 1.\n",
        "\n",
        "*\t**P-valor de los estadísticos t**: El p-valor es un parámetro que está asociado a cada uno de los coeficientes (un p-valor para cada coeficiente), aunque tambien se obtiene un p-valor global para el modelo de regresión completo. Los p-valores indicarán si el coeficiente $\\beta_j$ de cada variable de entrada $X_j$ es relevante, o si por el contrario hay una alta probabilidad de que realmente pudiese ser cero ($\\beta_j \\approx 0$ implica que es una variable sin relación lineal con la variable de salida, y por lo tanto a eliminar del modelo). Esto pasa cuando tenemos un p-valor para dicho coeficiente por encima de $0,1$ o $0,15$, que indicaría que la variable podría no ser relevante ($\\beta_j \\approx 0$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSIafL_qGTfK"
      },
      "source": [
        "### **1.4 Evaluación de la bondad del modelo**\n",
        "\n",
        "Como ya se mencionó en la cápsula anterior, el valor del **Coeficiente de Determinación $R^2$** es un valor que puede estar entre 0 y 1, indicando el valor 1 un ajuste perfecto del modelo (error cero), y el 0 un ajuste con el peor error posible. Para **comparar varios modelos de regresión lineal entre sí**, se suele atender al valor de **$R^2$** obtenido en cada uno de ellos. No obstante, como se debe de tener en cuenta el número de coeficientes $\\beta_j$ de los distintos modelos, para poder comparar modelos con distinto número de variables de entrada, en realidad, tendríamos que mirar el conocido como **$R^2$ ajustado** (que ya los tiene en cuenta durante su cálculo).\n",
        "\n",
        "Por otra parte, si queremos comparar un **modelo de regresión lineal** con modelos obtenidos por **otras técnicas de regresión** (Knn, Redes neuronales, M5, ...) **se utilizará el valor de RECM**. Recordemos que, por el contrario, **$R^2$** es un parámetro relativo y que además no todas las técnicas permiten su cálculo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az-s7fRCGZ4K"
      },
      "source": [
        "### **1.5 Elección de las variables de entrada relevantes**\n",
        "\n",
        "La utilidad de la regresión lineal depende mucho de las variables que se escojan para aprender sus **coeficientes**. En principio, no hay otra forma que seleccionarlas de manera manual, ya que no se pueden examinar todas las posibles combinaciones de variables. A modo de ejemplo, para un problema **con 40 variables de entrada**, $p = 40$, tendríamos $2^p$ combinaciones de modelos distintos **(por encima de un billón de combinaciones)**. **Para decidir qué variables deberían ser consideradas en el modelo final** existen **dos enfoques** comúnmente utilizados:\n",
        "\n",
        "1.\t**Selección ascendente**: Este procedimiento consiste en comenzar construyendo el **modelo nulo** - un modelo que contiene el término independiente pero ninguna variable de entrada - y realizar en paralelo regresiones lineales simples entre la variable de salida/dependiente $Y$ y cada uno de las variables de entrada $X_j$. A continuación, se agregará al modelo nulo la variable de entrada que resulte en **el error más bajo entre todos los modelos testeados (aquel con el $R^2$ ajustado más alto)**. Continuaremos con este procedimiento hasta que se cumpla alguna regla de detención, por ejemplo, cuando al añadir cualquiera de las variables restantes **se obtenga un p-valor por encima de algún umbral determinado para dicha variable ($>0,1$ o $0,15$ por ejemplo)**. **¡¡¡IMPORTANTE!!!:** *Las variables deben de ser añadidas de una en una y NUNCA varias de golpe. Cada vez que se añade una variable, ésta contribuye a explicar una parte de los datos, por lo que los p-valores de las demás podrían cambiar y dejar de ser relevantes al ya no haber necesidad de explicar dicha parte de los datos*.\n",
        "\n",
        "2.\t**Selección descendente**: Empezar con todas las variables en el modelo. Eliminar la variable de entrada con el p-valor más alto, es decir, la variable estadísticamente menos significativa y ajustar el nuevo modelo con las $(p - 1)$ variables restantes. En la siguiente vuelta, de nuevo, la variable con el mayor p-valor se elimina, continuando con el proceso hasta que se alcance una regla de parada. Por ejemplo, podemos detenernos cuando todas las variables de entrada que componen el modelo tengan un valor significativo (por ejemplo p-valor por debajo de $0.1$ o $0.15$). **¡¡¡IMPORTANTE!!!:** *Las variables deben de ser eliminadas de una en una y NUNCA varias de golpe. Cada vez que se elimina una variable estamos facilitando que alguna de las que siguen quedando pueda explicar de manera adecuada una parte de los datos, pasando de ser una variable poco relevante o incluso molesta (con un p-valor muy alto) a una variable imprescindible. Por lo tanto, perderíamos esas variables si quitamos varias de golpe. Si ésto se sigue a rajatabla, se solucionará buena parte de los problemas de correlación (colinealidad) entre variables de entrada.*\n",
        "\n",
        "Es importante considerar que, en cualquiera de estos dos enfoques, el **término independiente** de la regresión no entra en juego, y por lo tanto nunca se eliminará del modelo.\n",
        "\n",
        "Existen otras posibilidades además de estos dos enfoques, como por ejemplo calcular las correlaciones que todas las variables de entrada tienen con la variable de salida, y quedarse con un conjunto pequeño de las mejores; o por ejemplo, aplicar alguno de los algoritmos de selección de variables existentes. **Por desgracia, ninguna de ellas asegura que se llegue a la mejor combinación existente**.\n",
        "\n",
        "En este curso, **recomendamos utilizar la selección descendente cuando el número de variables no sea excesivamente elevado**, y **la ascendente cuando si lo sea** (automatizando la elección de la variable que debe incluirse en cada paso). \n",
        "\n",
        "El uso de cualquiera de estos dos enfoques nos permitirá justamente lo que vamos buscando con la regresión: **Estudiar un conjunto de datos y poder sacar conclusiones últiles** sobre el comportamiento de los mismos, incluso aunque finalmente se apliquen otro tipo de técnicas más versátiles. En nuestro  **conjunto de datos sobre la obesidad e insulino-resistencia en niños**, aplicaremos **el enfoque descendente**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K2muufPGdx7"
      },
      "source": [
        "### **1.6 Extensiones del modelo lineal. Eliminando la suposición aditiva: interacciones y no linealidad.**\n",
        "\n",
        "Para atender a la **no linealidad** de los datos existente en la mayoría de conjuntos de datos de la vida real, se pueden considerar nuevos **términos de interacción** (variables con sinergia positiva que se potencian la una a la otra) o de **no linealidad** (variables con crecimiento cuadrático, logarítmico, etc.). Para poder estudiar estos comportamientos en una **regresión lineal** haremos uso de dos figuras:\n",
        "\n",
        "* **Interacciones** (términos que no presentan un comportamiento aditivo entre sí): Estos términos de interacción se presentan en los modelos de regresión como **$X_1$ $*$ $X_2$**, y representan como el cambio en dos o más variables de manera conjunta provoca cambios en la variable de salida $Y$ mayores de lo que lo harían por separado. Por ejemplo: es conocido que la inversión de 5000 euros de publicidad en $radio$ y otros 5000 en $televisión$ provoca ventas mucho mayores de un producto que si se invierte directamente 10000 euros en cualquiera de los dos medios de manera unilateral. Incluir en el modelo de regresión un nuevo término multiplicativo del tipo $radio*televisión$ nos permitiría explicar adecuadamente dicho tipo de fenómeno no lineal.\n",
        "\n",
        "* **Otros términos no lineales**: Muchas veces la relación entre una variable de entrada y la variable de salida no es lineal sino cuadrática, cúbica, logarítmica, exponencial, etc. Incluir un término que coincida con dicho tipo de relación puede ayudar a explicar adecuadamente estos fenómenos no lineales.\n",
        "\n",
        "No obstante, en todos los casos se debe de respetar el **principio de jerarquía**. Es decir, si se incluye una variable nueva $X_5^3$ a partir de $X_5$ y su p-valor indica que dicho término cúbico es relevante, entonces se deben incluir también $X_5^2$ y la propia $X_5$, incluso si sus p-valores son altos. **En caso contrario, estaremos cometiendo errores graves**. Es igual en el caso de las interacciones. Si se incluye y se mantiene una variable nueva $(X_1 * X_2 * X_6)$ porque las tres se complementan y, por lo tanto, el p-valor para dicho término indica que es relevante, entonces se deben incluir también (independientemente de sus p-valores): $(X_1 * X_2)$, $(X_1 * X_6)$, $(X_2 * X_6)$, $X_1$, $X_2$, y $X_6$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m39pteJyR2G-"
      },
      "source": [
        "## **2. INSTALACIÓN DE R, BIBLIOTECAS Y LECTURA DE LOS DATOS DE OBESIDAD INFANTIL <a id='etiquetaB'></a>**\n",
        "\n",
        "Como se explicó en la cápsula anterior, necesitamos ejecutar las siguientes 3 celdas antes de empezar con el algoritmo de regresión lineal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU5gcUFGSpZl"
      },
      "source": [
        "# Tiempo estimado de ejecución: 20 segundos aprox.\n",
        "\n",
        "### Instalación de R en notebooks de Google Colab ###\n",
        "!apt-get update\n",
        "!apt-get install r-base\n",
        "!pip install rpy2\n",
        "%load_ext rpy2.ipython\n",
        "print (\"Instalación de R en Google Colab terminada\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJX4puBixZ0L"
      },
      "source": [
        "# Tiempo estimado de ejecución: 4 segundos aprox (al no necesitar importar kknn y Cubist).\n",
        "# Bibliotecas necesarias:\n",
        "# ISLR para regresión lineal multivariable\n",
        "# kknn para k-vecinos más cercanos de regresión\n",
        "# Cubist para modelos de regresión basados en M5\n",
        "\n",
        "%%R\n",
        "### Instalación de las bibliotecas necesarias\n",
        "#install.packages(c(\"ISLR\", \"kknn\", \"Cubist\"))\n",
        "install.packages(c(\"ISLR\")) #kknn y Cubist se utilizarán en la siguiente cápsula\n",
        "print (\"Instalación de las bibliotecas de R para este módulo terminada\")\n",
        "\n",
        "### Importación de las bibliotecas necesarias ###\n",
        "require(ISLR)\n",
        "#require(kknn)\n",
        "#require(Cubist)\n",
        "print (\"Importación de las bibliotecas de R para este módulo terminada\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSwe0RnTJOx"
      },
      "source": [
        "# Tiempo estimado de ejecución: 2 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Lectura\n",
        "data <- read.csv(url(\"https://drive.google.com/uc?id=1GO2NBxYw54K6HkN-YgXbNadrLo5O6-0u\"))\n",
        "\n",
        "### Visualización de una pequeña parte de los datos\n",
        "head(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VQ9wzzqm-eA"
      },
      "source": [
        "Como podemos observar, el comando *head* nos ofrece una visualización de los datos disponibles para los primeros 6 individuos/instancias del conjunto. En esta visualización, podemos identificar variables como el *sexo* de los individuos (el cual viene codificado como 0 si el individuo es un varón, o 1 si es una niña), el *estadío puberal* (representado por la variable \"Tanner\", y codificado como 0 para el estadío pre-puberal y 1 para el estadío puberal), o la *presión sanguínea* (representada por las variables \"DBP\" para la tensión diastólica, y \"SBP\" para la tensión sistólica). Como se puede observar, también se cuenta con variables de sedentarismo, y actividad física ligera, moderada y vigorosa. El resto de variables abreviadas se refieren a: *BMI*, Body Mass Index (o Índice de Masa Corporal traducido al Español); *WC*, Waist Circumference (o Circunferencia de cintura traducido al Español); *TAG*, tliglicéridos; *HDL*, high-density lipoprotein (Colesterol \"Bueno\"); *LDL*, Low-density lipoprotein (Colesterol \"Malo\"); estos tres últimos expresados en miligramos/decilitro en sangre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-2jsgK3pCqm"
      },
      "source": [
        "## **3. PRIMERA TOMA DE CONTACTO CON EL PROBLEMA Y ESTUDIO DE LAS VARIABLES DE MAYOR INTERÉS <a id='etiquetaC'></a>**\n",
        "\n",
        "Una vez importadas las librerías y leídos los datos estamos en disposición de ver **qué variables son las más prometedoras** para **aplicar la regresión lineal** en este **problema**. Para estudiar qué variables explican mejor el comportamiento de la variable de salida *HOMA-IR*, se podrían **cálcular las correlaciones** existentes entre ésta y cada una de las variables de entrada (lo cual se lleva a cabo mediante el comando: *cor(data)*). De esta manera, se podrían escoger aquellas que se encuentren más correladas.\n",
        "\n",
        "Si llevamos a cabo este procedimiento sobre nuestro conjunto de datos, observaremos cómo $SBP$ tiene una correlación más alta con el *HOMA-IR* que otras variables de entrada (por ejemplo $Sex$). Sin embargo, al final de nuestro estudio veremos que $SBP$ no terminará formando parte del modelo final, al contrario que $Sex$ que si entrará como variable seleccionada. La explicación para esto es que la construcción del modelo final no sólo depende de la correlación individual de cara variable de entrada con la variable de salida, sino más bien de lo que aporte cada variable con respecto al resto de variables seleccionadas. Si lo que puede explicar una variable de entrada ya está mejor explicado por otra, ésta no debe formar parte del modelo final.\n",
        "\n",
        "Una alternativa a lo anterior es **mostrar gráficamente la relación de cada variable de entrada con respecto a la variable de salida** *HOMA-IR*. De esta manera, podemos observar visualmente no sólo si presentan una relación más o menos lineal, sino qué forma tiene la nube de puntos. Por ejemplo, podríamos ver si una variable presenta un comportamiento cuadrático o logarítmico, y por lo tanto sería más apropiado incluir estos términos en el modelo. \n",
        "\n",
        "En nuestro caso de estudio, optaremos por esta **segunda aproximación**, por considerarla tanto o más informativa. El siguiente bloque de código *R* grafica iterativamente, y por orden, todas las variables de entrada con respecto a la variable de salida (*HOMA-IR*).\n",
        "\n",
        "**NOTA:** *A partir de aquí es importante que también lea los comentarios incluidos junto con el código para una mejor comprensión*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRSV9yH8omFJ"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Visualización de las variable respecto a HOMA\n",
        "temp <- data\n",
        "plotY <- function (x,y) {\n",
        "\tplot(temp[,y]~temp[,x], xlab=paste(names(temp)[x],\" X\",x,sep=\"\"), ylab=names(temp)[y])\n",
        "}\n",
        "par(mfrow=c(4,4)) #Si margin too large => (5,3)\n",
        "x <- sapply(1:(dim(temp)[2]-1), plotY, dim(temp)[2])\n",
        "par(mfrow=c(1,1))\n",
        "\n",
        "#cor(data) # Descomentar si queremos ver los valores concretos de correlación"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4Hq3GIwp6Uv"
      },
      "source": [
        "Como resultado, podemos ver como las variables $BMI$, $WC$ y $Height$ parecen ser las más prometedoras (ya que muestran una relación relativamente lineal con el $HOMA$), a pesar de mostrar cierta dispersión en los datos. Esto último es una señal de que no hay un único factor explicativo de valor de la insulino-resistencia ($HOMA$). Además, se puede ver que las tres presentan cierta no linealidad, siendo este comportamiento más notorio para el $BMI$, el cual parece mostrar cierta relación cuadrática. En los siguientes bloques de código, nos centraremos en estas tres variables y aplicaremos una regresión lineal simple con cada una de ellas como una primera toma de contacto. Los siguientes bloques de código lanzan una regresión lineal simple entre el $HOMA$ y $BMI$, $Height$ y $WC$ respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm0GUiivp9mi"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Obteción del modelo. Función lm() del paquete ISLR.\n",
        "### Y=HOMA, X's=BMI (índice de masa corporal) -> formula: HOMA ~ BMI\n",
        "fitLM <- lm(HOMA ~ BMI, data=data) \n",
        "\n",
        "### Visualización de la línea (azul, valores estimados) vs valores reales (negro, valores observados). \n",
        "yprime = predict(fitLM,data)\n",
        "plot(data$HOMA~data$BMI)\n",
        "points(data$BMI,yprime,col=\"blue\",pch=20)\n",
        "\n",
        "### Coeficientes (Estimate), p-valores (Pr(>|t|)), R2 ajustado (Adjusted R-squared),\n",
        "### estadístico F y p-valor (F-statistic y p-value)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKPGE1q1uC5I"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Idem para la variable Height (altura)\n",
        "fitLM <- lm(HOMA ~ Height, data=data) \n",
        "yprime = predict(fitLM,data)\n",
        "plot(data$HOMA~data$Height)\n",
        "points(data$Height,yprime,col=\"blue\",pch=20)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06LxPkXAudFS"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Idem para la variable WC (circunferencia de la cintura)\n",
        "fitLM <- lm(HOMA ~ WC, data=data) \n",
        "yprime = predict(fitLM,data)\n",
        "plot(data$HOMA~data$WC)\n",
        "points(data$WC,yprime,col=\"blue\",pch=20)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoMmSKq6vXVs"
      },
      "source": [
        "Como resultado, podemos observar que los **p-valores** asociados a los **coeficientes** de las tres variables (columna con nombre \"$Pr(>|t|)$\") indican claramente que las **tres están relacionadas con la insulino-resistencia** (ya que adquieren p-valores muy por debajo de 0,1). Si bien es el modelo de regresión basado en ($BMI$) el que explica más variabilidad del $HOMA$ (de acuerdo a su valor de **$R^2$ ajustado**), es cierto que no presenta un valor muy alto (0,3458). \n",
        "\n",
        "\n",
        "Alternativamente, repetiremos el proceso de selección de variables pero esta vez llevando a cabo el mencionado **enfoque descendente**, el cual es aplicable ya que sólo contamos con 15 variables de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_xiNPFhpCqh"
      },
      "source": [
        "## **4. SELECCIÓN ADITIVA DE VARIABLES: ENFOQUE DESCENDENTE <a id='etiquetaD'></a>**\n",
        "\n",
        "A continuación, dejamos atrás las **regresiones lineales simples** y pasamos a considerar el modelo de **regresión múltiple**. Como ya se indicó, seguiremos un enfoque de **selección de variables descendente**. En los siguientes bloques de código se muestran uno a uno los pasos realizados para que se pueda hacer un seguimiento de las decisiones tomadas en cada momento. \n",
        "\n",
        "Tal y como hemos explicado en las secciones anteriores, la selección de variables mediante **enfoque descendente** se lleva a cabo incluyendo todas las variables en el modelo. Esto se consigue en *R* mediante el comando: (*Y ~ .*), donde el punto es la forma de indicar \"*todas las variables de entrada disponibles en el conjunto de datos*\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V97D_p3Nwwwg"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Obteción del modelo. Y=HOMA, X's=Todas -> formula: HOMA ~ .\n",
        "fitLM <- lm(HOMA ~ ., data=data) \n",
        "\n",
        "### Recordatorio:\n",
        "### Coeficientes (Estimate), p-valores (Pr(>|t|)), R2 ajustado (Adjusted R-squared),\n",
        "### estadístico F y p-valor (F-statistic y p-value)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7zhKviPxx8Y"
      },
      "source": [
        "Una vez obtenido el modelo de **regresión lineal múltiple** con todas las variables de entrada, hay un punto muy importante que no debemos pasar por alto. Este es el valor obtenido para el **estadístico F**, el cual debe ser lo primero que comprobemos. Como se explicó en los conceptos básicos, si su valor es cercano a 1 y/o paralelamente su ***p-valor*** está por encima de $0,1$ o $0,15$ no existiría ninguna variable que presente relación lineal con la variable de salida *HOMA-IR*. Esta interpretación **siempre será independientemente del p-valor individual** obtenido para cada coeficiente, el cual podría engañarnos hasta que no vayamos eliminando variables de entrada redundantes o no informativas. De darse esa situación, se detendría el análisis de regresión lineal y buscaríamos otra técnica de regresión alternativa. Como ya sabíamos por los valores obtenidos en el apartado anterior, este no es el caso de nuestro conjunto de datos.\n",
        "\n",
        "Chequeando los **p-valores** obtenidos tras este primer paso, podemos ver que el siguiente paso sería eliminar la variable $Age$ por presentar el p-valor más alto, 0,89656. Un detalle interesante es que el **$R^2$ ajustado** del modelo completo mejora con respecto a los **$R^2$** obtenidos para los modelos de regresión lineal simple del apartado anterior (alcanzando un valor de 0,4876). Para eliminar la variable $Age$ del modelo completo lo hacemos utilizando el comando *R* de signo de sustracción \"-\" en la fórmula, obteniendo así el nuevo modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-_a6XPoT4x5"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Obteción del modelo. Y=HOMA, X's=Todas-Age -> formula: HOMA ~ .-Age\n",
        "fitLM <- lm(HOMA ~ .-Age, data=data) \n",
        "\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCjodoRGUc-_"
      },
      "source": [
        "Tras eliminar $Age$, se puede ver cómo el nuevo **$R^2$ ajustado** del modelo incluso mejora. Esto es el resultado de eliminar una variable que no aportaba nada al modelo. El siguiente paso, en vista de los resultados obtenidos, será eliminar la variable de actividad física $Moderate$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkRGZBvbTjZD"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Idem al anterior -Moderate\n",
        "fitLM <- lm(HOMA ~ .-Age-Moderate, data=data) \n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwGxVz2bVnDn"
      },
      "source": [
        "Conforme se van eliminando variables no informativas, vemos como el valor **$R^2$ ajustado** del modelo resultante sigue mejorando poco a poco. La siguiente variable a eliminar es $Vigorous$ (referida a los minutos diarios de actividad física vigorosa de los sujetos). \n",
        "\n",
        "En este punto queda perfectamente claro como debe acometerse el proceso de ir eliminando variables no informativas de una en una. Siempre debemos hacerlo así aunque resulte tedioso. En lo que sigue se mostrarán los siguientes pasos comentados hasta llegar al último paso (modelo final), el cual si que ejecutaremos para ver a qué se llega finalmente. Por favor, recuerde leer también los comentarios en línea con detenimiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W9SAxb0XGIG"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "#fitLM <- lm(HOMA ~ .-Age-Moderate-Vigorous, data=data)\n",
        "#summary(fitLM)\n",
        "#fitLM <- lm(HOMA ~ .-Age-Moderate-Vigorous-SBP, data=data)\n",
        "#summary(fitLM)\n",
        "#fitLM <- lm(HOMA ~ .-Age-Moderate-Vigorous-SBP-Tanner, data=data)\n",
        "#summary(fitLM)\n",
        "\n",
        "### En el modelo anterior ya todos los p-valores se podrían considerar correctos.\n",
        "### Por simplicidad hemos seguido quitando mientras el R2 ajustado apenas se ha visto afectado.\n",
        "#fitLM <- lm(HOMA ~ .-Age-Moderate-Vigorous-SBP-Tanner-Light, data=data)\n",
        "#summary(fitLM)\n",
        "#fitLM <- lm(HOMA ~ .-Age-Moderate-Vigorous-SBP-Tanner-Light-Sedentary, data=data)\n",
        "#summary(fitLM)\n",
        "\n",
        "### A partir de aquí R2 empezaría a empeorar significativamente.\n",
        "### Paramos y reformulamos por legibilidad indicando las variables de entrada seleccionadas de manera aditiva\n",
        "### Este modelo es equivalente al inmediatamente anterior pero muestra con claridad lo seleccionado\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+HDLCmgDL+DBP, data=data) #Vea que ya no se incluye el punto\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXv4ClDmCf6v"
      },
      "source": [
        "Como resultado, obtenemos un modelo con 8 variables de entrada y un **$R^2$ ajustado** de 0,4861."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nzOIPW-FNRp"
      },
      "source": [
        "## **5. INTERACCIONES Y NO LINEALIDAD <a id='etiquetaE'></a>**\n",
        "\n",
        "Una vez hemos seleccionado las variables de entrada que deben incorporarse a nuestro **modelo lineal**, intentaremos poder explicar la parte **no lineal** de los datos mediante la adición de **interacciones y otros términos no lineales**. Para probar interacciones nos basamos en el conocimiento previo que tengamos del problema (por ejemplo, en un caso de interacción genética conocida entre dos variantes genéticas, sería apropiado introducir un término de interacción entre ambas para modelar su efecto sobre la variable de salida). En caso de que no haya información previa sobre fenómenos de interacción, también podremos guiarnos por la lógica o intuición según el significado de las variables de entrada. Si aún así no caemos en ninguna posible interacción, podemos hacer pruebas aleatorias entre las variables que se han mostrado más significativas (ensayo-error). Este procedimiento no es un proceso trivial y depende de nuestra propia habilidad y experiencia. \n",
        "\n",
        "En nuestro caso de estudio sobre obesidad infantil, vamos a comprobar si existe sinergia positiva (factores multiplicativos, $*$) entre la variable de trigliceridos y las dos medidas de colesterol (por pertenecer todas ellas al perfil lipídico)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy2I45sceIcD"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Interacciones entre triglicéridos y colesteroles\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+HDLCmgDL+DBP+TAGmgDL*HDLCmgDL*LDLCmgDL, data=data)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ_TfJuCjBkp"
      },
      "source": [
        "Véase que el uso de operadores ya incluye todos los términos de jerarquía. Si no lo hiciese, tendríamos que añadirlos a mano antes de mirar ningún p-valor, ni de tomar decisión alguna. Podemos ver cómo el término *TAGmgDL:LDLCmgDL:HDLCmgDL* presenta un p-valor realmente malo (0,961424), indicando que no existe dicha interacción hipotetizada.\n",
        "\n",
        "Probaremos de nuevo con la altura y la circunferencia de la cintura."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BNEmOCrkvNl"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Interacciones entre altura y la circunferencia de la cintura\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+HDLCmgDL+DBP+Height*WC, data=data)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K-6keAclEDg"
      },
      "source": [
        "En este caso si podemos ver cómo la interacción explica parte de esa **no linealidad** (al obtener un p-valor inferior a 0.1). En principio nos la quedamos como parte del modelo.\n",
        "\n",
        "Por último, probaremos otros términos de **no linealidad**. En este caso, teniendo en cuenta que inicialmente graficamos todas las variables de entrada respecto a la variable de salida *HOMA-IR*, resulta bastante más sencillo determinar ciertos tipos de comportamientos no lineales de manera visual. En concreto, vimos que la variable de salida $HOMA-IR$ parecía tener una relación cuadrática con $BMI$. Probaremos, por lo tanto, a incluir dicho término. La forma de hacerlo es mediante la función $I(.)$ de $R$. La potencia se indica de la siguiente manera: $I(X_j \\hat{} exponente)$. En nuestro caso $I(BMI \\hat{} 2)$. La función $I(.)$ no genera automáticamente los términos de jerarquía, por lo que antes de mirar siquiera el modelo debemos asegurarnos de que todos los términos de jerarquía están en la fórmula. Para el caso que nos aplica, los terminos de jerarquía serían: $BMI + BMI^2 + Height + WC + Height*WC$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-ZqqyMGnjiJ"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "### Interacciones entre altura y la circunferencia de la cintura, más BMI^2\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+HDLCmgDL+DBP+Height*WC+I(BMI^2), data=data)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PwFH_Qqn_ri"
      },
      "source": [
        "De nuevo, aparecen términos con p-valores altos, como es el caso de $Height*WC$. La interacción estaba intentando explicar lo que ahora el término $BMI$ cuadrático explica mejor. Quitaremos por lo tanto la interacción ahora no significativa. Recordemos que la variable BMI ya mostraba claramente un aspecto no lineal cuadrático en el gráfico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLYEM1YoosS-"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+HDLCmgDL+DBP+I(BMI^2), data=data)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtwcbVJppDHT"
      },
      "source": [
        "De acuerdo al nuevo modelo, ahora habría que eliminar $DBP$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgYXwNwrpKx5"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+HDLCmgDL+I(BMI^2), data=data)\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj47zEYZpiS7"
      },
      "source": [
        "Y por último $HDLCmgDL$. \n",
        "\n",
        "En una línea de código adicional, incluimos también cómo calcular la métrica **RECM** para un modelo de regresión lineal (la cual no aparecía en el *output* de resultados ofrecido por el comando *summary*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG93hsxBpZ6K"
      },
      "source": [
        "# Tiempo estimado de ejecución: 3 segundos aprox.\n",
        "\n",
        "%%R\n",
        "fitLM <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+I(BMI^2), data=data)\n",
        "\n",
        "### Cálculo de RECM\n",
        "yprime = predict(fitLM,data)\n",
        "cat('\\nRMSE:', sqrt(sum((data$HOMA-yprime)^2)/length(yprime)), \"\\n\") #RECM->en inglés RMSE\n",
        "\n",
        "summary(fitLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wWD9aFkvCBD"
      },
      "source": [
        "Visualización:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "872Sn6zDvE6a"
      },
      "source": [
        "#yprime = predict(fitLM,data)\n",
        "\n",
        "%%R\n",
        "plot(data$HOMA~data$BMI)\n",
        "points(data$BMI,yprime,col=\"blue\",pch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2s8LG84qBrD"
      },
      "source": [
        "Finalmente, hemos llegado hasta un **$R^2$ ajustado** de 0,6092 cuando partíamos de 0,3458. Pero lo más importante no es dicho valor en sí, sino lo que hemos podido aprender sobre los datos y de nuestro problema basándonos en valores estadísticos. Como principal conclusión, podemos extraer que un alto índice de masa corporal en niños es uno de los principales factores de riesgo para padecer insulino-resistencia. Pensemos que incluso aunque la conclusión fuese que tenemos que replantear el problema con nuevas variables y mediciones, ya es un gran paso el poder darse cuenta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhpdZTnJmlTd"
      },
      "source": [
        "## **6. VALIDACIÓN CRUZADA <a id='etiquetaF'></a>**\n",
        "\n",
        "Una vez que ya tenemos planteada la mejor fórmula para aplicar el ajuste paramétrico (modelo de regresión), si quisieramos estimar nuevos valores de la variable de salida y comparar su habilidad predictiva con otros modelos deberíamos aplicar una validación cruzada por los motivos que se explicaron en el Módulo 3 de este $MOOC$. A continuación, se muestra como poder hacerlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmS76-Svu2o7"
      },
      "source": [
        "%%R\n",
        "set.seed(123456)\n",
        "k <- 5            \n",
        "data$kfold <- sample(1:k, nrow(data), replace = T)\n",
        "\n",
        "performances <- c()\n",
        " \n",
        "# One iteration per fold\n",
        "for (fold in 1:k){\n",
        "  # Se crea el conjunto de entrenamiento para la iteración\n",
        "  training_set <- data[data$kfold != fold,]\n",
        "  nombres <- names(training_set)\n",
        "  tam <- length(nombres)-1\n",
        "  training_set <- training_set[,nombres[1: tam]]\n",
        "    \n",
        "  # Create test set for this iteration\n",
        "  # Subset all the datapoints where .folds matches the current fold\n",
        "  testing_set <- data[data$kfold == fold,]\n",
        "  nombres <- names(testing_set)\n",
        "  tam <- length(nombres)-1\n",
        "  testing_set <- testing_set[,nombres[1: tam]]\n",
        "\n",
        "  ## Entrenando el modelo para la iteración\n",
        "  model <- lm(HOMA ~ BMI+Height+TAGmgDL+Sex+WC+LDLCmgDL+I(BMI^2), data=training_set)\n",
        "\n",
        "  ## Calculando el error de test\n",
        "  yprime <- predict(model, testing_set)\n",
        "  RMSE <- sqrt(sum((testing_set$HOMA-yprime)^2)/length(yprime))\n",
        "\n",
        "  # Add the RMSE to the performance list\n",
        "  performances[fold] <- RMSE\n",
        "}\n",
        "\n",
        "#Eliminamos la columna artificial añadida para kfold\n",
        "#(para que no acumule columnas si se ejecuta varias veces)\n",
        "nombres <- names(data)\n",
        "tam <- length(nombres)-1\n",
        "data <- data[,nombres[1: tam]]\n",
        "\n",
        "cat(\"RECM medio en test para 5-fcv:\", mean(performances))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAsoqNeHo2nF"
      },
      "source": [
        "## **REFERENCIAS BIBLIOGRÁFICAS <a id='etiquetaG'></a>**\n",
        "\n",
        "-\tGareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.\n",
        "An Introduction to Statistical Learning with Applications in R\n",
        "Springer, 2013 (**Chapter 03**)\n",
        "-\tMcDonald, J.H. Handbook of Biological Statistics (3rd ed.). Sparky House Publishing, Baltimore, Maryland, 2014. Pages 190-208 in the printed version\n",
        "- Usando rpy2 en notebooks: https://rpy2.github.io/doc/latest/html/notebooks.html\n",
        "- Usando read.csv de R: https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/read.table\n",
        "- Usando ISLR: https://cran.r-project.org/web/packages/ISLR/index.html\n",
        "\n",
        "## **REFERENCIAS ADICIONALES**\n",
        "\n",
        "-\tM.J. Gacto, J.M. Soto-Hidalgo, J. Alcalá-Fdez, and R. Alcalá (2019). Experimental Study on 164 Algorithms Available in Software Tools for Solving Standard Non-Linear Regression Problems. IEEE Access 7, 2019, pp. 108916-108939; https://doi.org/10.1109/ACCESS.2019.2933261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_y70_DKpCqq"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "MOOC Machine Learning y Big Data para la Bioinformática (1ª edición)   \n",
        "\n",
        "http://abierta.ugr.es     \n",
        "</div>    "
      ]
    }
  ]
}