{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Modulo5Capsula3_abiertaUGR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vD-6nUjHFjP"
      },
      "source": [
        "![cabecera_slide_abiertaugr_bigdata.jpg](https://i.imgur.com/HXn24wC.jpg)\n",
        "## Módulo 5.3 Métodos avanzados en clasificación.\n",
        "\n",
        "**Autor**: \n",
        "\n",
        "*Por* Prof. Alberto Fernández Hilario\n",
        "\n",
        "Profesor Titular de Universidad de Granada. Instituto Andaluz Interuniversitario en Data Science and Computational Intelligence (DasCI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Y_zJaSHFjR"
      },
      "source": [
        " ## Breves Instrucciones\n",
        "\n",
        "### Recordatorio: Introducción a NoteBook\n",
        "\n",
        "El cuaderno de *Jupyter* (Python) es un enfoque que combina bloques de texto (como éste) junto con bloques o celdas de código. La gran ventaja de este tipo de celdas, es su interactividad, ya que pueden ser ejecutadas para comprobar los resultados directamente sobre las mismas. \n",
        "\n",
        "**Muy importante**: el orden de las instrucciones (bloques de código) es fundamental, por lo que cada celda de este cuaderno debe ser ejecutada secuencialmente. En caso de omitir alguna, puede que el programa lance un error (se mostrará un bloque salida con un mensaje en inglés de color rojo), así que se deberá comenzar desde el principio en caso de duda. Para hacer este paso más sencillo, se puede acceder al menú “Entorno de Ejecución” y pulsar sobre “Ejecutar anteriores”. \n",
        "\n",
        "¡Ánimo!\n",
        "\n",
        "Haga clic en el botón \"play\" en la parte izquierda de cada celda de código. Las líneas que comienzan con un hashtag (#) son comentarios y no afectan a la ejecución del programa.\n",
        "\n",
        "También puede pinchar sobre cada celda y hacer \"*ctrl+enter*\" (*cmd+enter* en Mac).\n",
        "\n",
        "Cuando se ejecute el primero de los bloques, aparecerá el siguiente mensaje: \n",
        "\n",
        "\"*Advertencia: Este cuaderno no lo ha creado Google.*\n",
        "\n",
        "*El creador de este cuaderno es \\<autor\\>@go.ugr.es. Puede que solicite acceso a tus datos almacenados en Google o que lea datos y credenciales de otras sesiones. Revisa el código fuente antes de ejecutar este cuaderno. Si tienes alguna pregunta, ponte en contacto con el creador de este cuaderno enviando un correo electrónico a \\<autor\\>@go.ugr.es.”*\n",
        "\n",
        "No se preocupe, deberá confiar en el contenido del cuaderno (Notebook) y pulsar en \"*Ejecutar de todos modos*\". Todo el código se ejecuta en un servidor de cálculo externo y no afectará en absoluto a su equipo informático. No se pedirá ningún tipo de información o credencial, y por tanto podrá seguir con el curso de forma segura. \n",
        "\n",
        "Cada vez que ejecute un bloque, verá la salida justo debajo del mismo. La información suele ser siempre la relativa a la última instrucción, junto con todos los `print()` (orden para imprimir) que haya en el código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PYSesP3atPV"
      },
      "source": [
        "## **ÍNDICE** \n",
        "\n",
        "En este *notebook*: \n",
        "1. Se presentarán algoritmos de clasificación más sofisticados en aras de un mayor rendimiento predictivo.\n",
        "2. Se describirán los modelos basados en Máquinas de Vectores Soporte, como una extensión de los modelos lineales.\n",
        "3. Se introducirán los algoritmos tipo \"ensemble\" (múltiples clasificadores) y en concreto se utilizará Random Forest como su mayor exponente.\n",
        "4. Se analizarán las ventajas e inconvenientes de estos nuevos modelos.\n",
        "5. Se mostrará cómo utilizar estos modelos avanzados mediante *Scikit-Learn* para Python.\n",
        "6. Se discutirá sobre la influencia de los hiperparámetros en este tipo de clasificadores.\n",
        "    \n",
        "Contenidos:\n",
        "1. Introducción    \n",
        "2. Máquinas de Vectores Soporte (SVM)    \n",
        "3. Ensembles y Random Forest     \n",
        "4. Referencias bibliográficas "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo3h6f7lH_Am"
      },
      "source": [
        "## **1. INTRODUCCIÓN**\n",
        "\n",
        "En esta primera sección, se llevará a cabo una presentación sobre los paradigmas de clasificación avanzados que se describirán a lo largo del presente Módulo. A continuación, se cargarán y almacerán en variables de Python los datos del problema sobre melanoma cutáneo con los que se viene trabajando hasta ahora. Además, este conjunto de datos se transformará reduciendolo a solamente dos dimensiones (variables de entrada) para realizar representaciones gráficas que ilustren el funcionamiento de cada técnica o algoritmo de clasificación en Machine Learning. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt5ax5BsblPY"
      },
      "source": [
        "### **1.1 Paradigmas de clasificación: modelos avanzados**\n",
        "\n",
        "Existen casos de estudio en los que se prima la máxima capacidad predictiva o acierto obtenido para la elección del modelo de clasificación. En este apartado, entrarían en juego los denominados \"modelos de caja negra\". Son algoritmos que obtienen un modelo de alto rendimiento, pero por contrapartida son complejos, es decir, no son directamente comprensibles por el usuario humano. En otras palabras, su comportamiento es robusto frente a problemas difíciles, aunque suelen implicar un mayor coste computacional en su aprendizaje (más recursos de CPU, memoria, más tiempo de ejecución, etc.), y necesitan un mayor nivel de conocimiento para su correcto uso.\n",
        "\n",
        "Se observa por tanto que las características de los modelos de \"caja blanca\" descritos en el anterior apartado del curso, como Regresión Logística o Árboles de Decisión, y estas nuevas soluciones, son prácticamente opuestas. Por ese motivo, es importante determinar cuándo resulta más adecuado utilizar cada tipo de técnica de Machine Learning, así como conocer bien sus características para así utilizarlos correctamente. \n",
        "\n",
        "En concreto, se describirán las Máquinas de Vectores Soporte (SVM) y los modelos tipo Ensemble, en concreto el conocido algoritmo Random Forest. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GDXE_2kHFjS"
      },
      "source": [
        "### **1.2 Cargar los datos del problema**\n",
        "\n",
        "\n",
        "Con el objetivo de comprobar el comportamiento de los diferentes algoritmos de clasificación, se comienza incorporando los datos del caso de estudio que sirve como hilo conductor del presente curso. La notación o código utilizado es exactamente el mismo que en actividades anteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FFxCHjDHFjT"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Cargamos los datos ómicos de la matriz de expresión desde un fichero compartido en Google Drive\n",
        "gene_exp_inmune = pd.read_csv('https://drive.google.com/uc?id=1PYzEIdmnfjOnBpPDIFBE9hL1Lkj_OBCk',index_col=0)\n",
        "#Cargamos la variable clínica correspondiente a las etiquetas \"inmune\" vs. \"MITF-low\"\n",
        "clinical_info_inmune = pd.read_csv('https://drive.google.com/uc?id=1hHQfcvrFa5Jds-9tW_X4sHjKpYKdii9s',index_col=0)\n",
        "\n",
        "X, y = gene_exp_inmune, clinical_info_inmune\n",
        "\n",
        "#Imprimimos las 5 primeras muestras del conjunto de datos para comprobar que se ha cargado correctamente\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkVSfjaU2K2h"
      },
      "source": [
        "Adicionalmente, tal como se realizó en la cápsula anterior (\"**Métodos estándar de clasificación**\"), para ilustrar el funcionamiento de cada clasificador, se procede a transformar el conjunto inicial de datos en un formato de 2 dimensiones (seleccionando únicamente dos variables de entrada). Es exactamente el mismo código que se incluyó en la cápsula anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-th95q0C--YZ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Transformamos el conjunto de datos inicial para que esté representado por solo 2 variables\n",
        "n_componentes = 2\n",
        "pca = PCA(n_components=n_componentes)\n",
        "X_2D = pca.fit_transform(X)\n",
        "  \n",
        "#Se transforma el rango de cada variable a [0, 1]\n",
        "st = StandardScaler()\n",
        "X_2D = st.fit_transform(X_2D)\n",
        "\n",
        "#Pintamos en un gráfico de puntos (scatterplot) el nuevo conjunto 2D\n",
        "plt.scatter(X_2D[:, 0], X_2D[:, 1], cmap=plt.cm.Set1, c=pd.get_dummies(y).iloc[:,0], edgecolor='k')\n",
        "plt.title(\"Representación 2D del problema de cáncer de melanoma\")\n",
        "plt.xlabel('PCA_V1')\n",
        "plt.ylabel('PCA_V2')\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5WEw_qg2ThQ"
      },
      "source": [
        "Por simplicidad, para los ejemplos incluidos en este NoteBook se utilizará una validación tipo \"*hold-out*\" por defecto. Para más detalles consúltese el **Módulo 3** sobre Aprendizaje Supervisado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxddBF3p2VWt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "y_int = pd.get_dummies(y).iloc[:,0]\n",
        "X_2D_train, X_2D_test, y_2D_train, y_2D_test = train_test_split(X_2D, y_int, random_state=42)\n",
        "\n",
        "print(\"Numero de instancias en entrenamiento: {}; y test: {}\".format(len(X_train),len(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka7UfXUf1Qeq"
      },
      "source": [
        "## **2. MÁQUINAS DE VECTORES SOPORTE (SVM)**\n",
        "\n",
        "Los modelos basados en **Máquinas de Vectores Soporte** (en inglés *Support Vector Machine o SVM*) son una de las herramientas preferidas por muchos científicos de datos. La razón es sencilla, ya que obtienen una alta precisión incluso en problemas complejos, utilizando relativamente poca potencia de cálculo. Adicionalmente, pueden ser empleados tanto para tareas de regresión como de clasificación, si bien es en esta última tarea donde mayor relevancia tiene.\n",
        "\n",
        "Este apartado, arranca con una introducción completa sobre las principales características y funcionamiento de las SVM. A continuación, se indica cómo hacer uso de este paradigma de clasificación mediante Scikit-Learn. Por último, se enumeran brevemente los pros y contras de este tipo de modelo. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTf3sCQDcJeS"
      },
      "source": [
        "### **2.1 Introducción a las SVM**\n",
        "\n",
        "El objetivo de una SVM es el de encontrar un hiperplano de separación entre las instancias de dos clases. Un hiperplano es exactamente el mismo tipo de función discriminante que se utilizó para otros clasificadores de tipo lineal, como por ejemplo la *regresión logística*. Recuérdese la ecuación del hiperplano, que no deja de ser un producto escalar de un vector de variables de entrada $x$ con un vector de pesos o importancia de las mismas $w$ $\\|x,w\\|$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}(x,w) = w_0 + w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n\n",
        "\\end{equation}\n",
        "\n",
        "La diferencia de SVM con respecto a otros métodos de separación lineal, es que entre todos los posibles hiperplanos que dividen las instancias en dos partes, se escoge aquél que obtiene un margen máximo. Este margen se calcula como la máxima distancia entre las instancias frontera, tal como se representa en la siguiente figura:\n",
        "\n",
        "![Margen calculado por la SVM](https://imgur.com/bNN5IAP.png)\n",
        "\n",
        "El nombre de esta técnica de aprendizaje viene determinado justamente por estas instancias en las que se \"apoya\" la frontera de decisión. Son los puntos más cercanos al hiperplano, e influyen directamente en su orientación para alcanzar el citado margen máximo. \n",
        "\n",
        "Existen diferentes mecanismos o aproximaciones matemáticas para encontrar la orientación óptima del hiperplano, si bien lo más importante es que dependen principalmente de la distancia de las instancias mal clasificadas (al otro lado de la \"frontera lineal\"). Por este motivo, el parámetro más importante de una SVM es el **Coste**, denominado como `C`:\n",
        "\n",
        "- Un valor bajo aceptaría cometer un cierto número de errores de clasificación, bajando ligeramente la calidad de predicción obtenida en el conjunto de entrenamiento, pero buscando una mejor generalización en test. \n",
        "- Un valor alto permite ajustar mejor el modelo sobre los datos de entrenamiento, pero implicaría un mayor riesgo de sobreaprendizaje. \n",
        "\n",
        "A pesar de lo anterior, un simple hiperplano no es la solución adecuada cuando las clases representadas en el problema no son linealmente separables (el problema es difícil), o cuando existe mucho ruido en los datos (se han cometido algunos errores en la captura de la información del problema). Sin embargo, se ha comentado que las SVMs son una herramienta que alcanza grandes resultados predictivos incluso en problemas complejos ¿cómo lo consigue?\n",
        "\n",
        "La solución empleada se conoce como *kernel trick*, y consiste básicamente en asignar los datos en un espacio más complejo, con variables no lineales, y usar el clasificador SVM lineal (la aproximación básica anteriormente descrita) en este nuevo espacio. Por ejemplo, añadiendo una nueva dimensión o variable, podemos encontrar una separación adecuada de los datos, como se aprecia en la siguiente figura: \n",
        "\n",
        "![transformación no lineal de los datos para crear una frontera lineal](https://i.imgur.com/AnUGAb7.png)\n",
        "\n",
        "Para cambiar el espacio de datos de las variables de entrada, se utilizan las denominadas *funciones kernel* (de ahí el nombre *kernel trick*), que son las incrementan la dimensionalidad del problema (\"añaden nuevas variables\"), haciendo transformaciones matemáticas no lineales. Los dos ejemplos más comunes de este tipo de funciones kernel son las siguientes:\n",
        "\n",
        "- Función polinomial: $K(x,w) = \\langle x, w \\rangle ^d$. De este modo, la sumatoria lineal de los productos de pesos $w_i$ y variables $x_i$ se transforma a un polinomio de mayor grado (2, 3, etc.). Así, en lugar de un hiperplano o \"línea recta\" tendremos una división de los datos más compleja.  \n",
        "- Función *Radial Base Function* (RBF): $K(x,w) = e^{-\\frac{\\|x-w\\|^2}{2\\cdot\\sigma}}$. Con el uso de funciones RBF las funciones discriminantes no-lineales en este caso se representan como áreas \"circulares\". \n",
        "\n",
        "No existe una respuesta universal sobre qué tipo de kernel utilizar, dependiendo mucho de las características del problema. Los kernel polinomiales son más sencillos, con menor tendencia al sobreaprendizaje cuando el grado es pequeño (menor o igual a 5). Los kernel RBF obtienen un mayor acierto en general, puesto que encuentran funciones discriminantes más complejas que permiten separar mejor las clases. Para ello se utiliza un parámetro conocido como \"gamma\" ($\\gamma$) que controla cómo se realiza la transformación del conjunto de datos, y por tanto valores altos tenderán al sobreaprendizaje.\n",
        "\n",
        "Encontrar la combinación ideal entre los parámetros de coste `C` y del kernel `d` o $\\gamma$ es una tarea bastante complicada, para la que se suelen utilizar procedimientos de ajuste de hiperparámetros como [Grid Search](https://scikit-learn.org/stable/modules/grid_search.html), cuyos detalles escapan a los objetivos del presente curso.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQbDzFPIfFZf"
      },
      "source": [
        "## **2.2 Implementación de las SVM en Python**\n",
        "\n",
        "Siguiendo el formato de todos los métodos de aprendizaje en *Scikit-Learn*, para ejecutar una SVM basta con crear un objeto de la clase `SVC` y ajustar utilizando los conjunto `X` e `y`, tal como se muestra a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0sxqfyj8mO6"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "clf = svm.SVC(probability=True) #parámetro para que se pueda calcular AUC\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "print(\"El porcentaje de acierto obtenido es\",acc*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F10KZ1UtizG0"
      },
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics.plot_confusion_matrix(clf, X_test, y_test,cmap='binary')\n",
        "plt.title(\"Matriz de confusión obtenida para el clasificador SVM\")\n",
        "plt.show()\n",
        "\n",
        "print(metrics.classification_report(y_test,y_pred))\n",
        "\n",
        "f1 = metrics.f1_score(y_test,y_pred,pos_label='immune')\n",
        "print(\"La medida F1 para el clasificador %s es %.4f\"%(clf.__class__.__name__,f1))\n",
        "\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
        "print(\"La medida AUC para el clasificador %s es %.4f\"%(clf.__class__.__name__,auc))\n",
        "metrics.plot_roc_curve(clf, X_test, y_test)\n",
        "plt.title(\"Curva ROC obtenida para el clasificador SVM\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX_s1q0n91vV"
      },
      "source": [
        "Se debe hacer énfasis en la especial importancia que tienen los parámetros de configuración en el algoritmo SVM. A continuación, se recuerdan cuáles son los principales y su notación específica en *Python*:\n",
        "\n",
        "- `C`: un valor real (`float`) que indica el parámetro de regularización o coste. Debe ser siempre positivo, donde mayores valores buscar un mejor ajuste a los datos de entrenamiento (por defecto=1.0).\n",
        "\n",
        "- `kernel`: un valor entre {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, que especifica el tipo de kernel usado en el algoritmo (por defecto ’rbf’).\n",
        "\n",
        "- `degree`: un valor entero (`int`) que indica el grado del kernel polinominal (`poly`) por lo que se ignora para el resto (por defecto=3). Es el parámetro anteriormente notado como `d`. \n",
        "\n",
        "- `gamma`: es un valor a elegir entre {‘scale’, ‘auto’} o un número real (`float`). Es el coeficiente del kernel para los casos ‘rbf’, ‘poly’ y ‘sigmoid’ (por defecto=’scale’, que equivale a `1 / (n_features * X.var())`; siendo ‘auto’ `1 / n_features`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-evSFTNYuPo"
      },
      "source": [
        "Por último, se muestran las fronteras generadas de acuerdo a diferentes kernel y valores del parámetro. Varios aspectos llaman la atención en este punto:\n",
        "\n",
        "1. Las fronteras representadas cuando se utiliza el kernel `rbf` suelen ser de tipo elíptico, mientras que para el polinomial (`poly`) presentan una tendencia de tipo \"lineal\".\n",
        "2. Al incrementar el parámetro `C` (SVM con RBF) usualmente se observan las fronteras más pegadas a los puntos de entrenamiento. \n",
        "3. Al incrementar el parámetro `degree` (SVM polinomial) se observan fronteras mucho más complejas.\n",
        "4. La representación gráfica de las funciones discriminantes en ocasiones parece que no coinciden con la posición de las instancias del conjunto de datos. Esto es totalmente natural, dado que en realidad la frontera se visualiza directamente sobre las nuevas variables que se construyen para poder encontrar una función discriminante lineal de separación entre los datos. \n",
        "\n",
        "Puesto que son muchos cálculos, puede que tarde unos segundos en mostrar los resultados. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5sWhz-xxNEI"
      },
      "source": [
        "#Se importa una biblioteca especial para pintar en 2D\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.gridspec as gridspec\n",
        "import itertools\n",
        "\n",
        "#Creamos y entrenamos los clasificadores SVM con los datos 2D\n",
        "svm_rbf_1 = svm.SVC(kernel='rbf',C=1)\n",
        "svm_rbf_100 = svm.SVC(kernel='rbf',C=100)\n",
        "svm_poly_2 = svm.SVC(kernel='poly',degree=2,C=1)\n",
        "svm_poly_5 = svm.SVC(kernel='poly',degree=5,C=1)\n",
        "svms = [svm_rbf_1, svm_rbf_100, svm_poly_2, svm_poly_5] #lista\n",
        "\n",
        "#Parámetros que se utilizarán para visualizar la figura\n",
        "scatter_kwargs = {'s': 120, 'edgecolor': None, 'alpha': 0.7}\n",
        "contourf_kwargs = {'alpha': 0.2}\n",
        "scatter_highlight_kwargs = {'s': 120, 'label': 'Test data', 'alpha': 0.7}\n",
        "\n",
        "#Necesario para pintar las 4 gráficas juntas\n",
        "gs = gridspec.GridSpec(2, 2)\n",
        "fig = plt.figure(figsize=(15,12))\n",
        "\n",
        "identificador = ['SVM RBF C1', 'SVM RBF C100', 'SVM Poly D2', 'SVM Poly D5']\n",
        "for clf, etq, grd in zip(svms, identificador, itertools.product([0, 1], repeat=2)):\n",
        "    clf.fit(X_2D_train, y_2D_train)\n",
        "    score = clf.score(X_2D_test,y_2D_test)\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X=X_2D, y=y_int.to_numpy().ravel(), clf=clf, \n",
        "                                X_highlight=X_2D_test, legend=2,\n",
        "                                scatter_kwargs=scatter_kwargs,\n",
        "                                contourf_kwargs=contourf_kwargs,\n",
        "                                scatter_highlight_kwargs=scatter_highlight_kwargs)\n",
        "    plt.title('Frontera de decisión generada por el clasificador '+etq)\n",
        "    plt.text(4 - .3, -3 + .3, ('Acc tst: %.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
        "    handles, labels = fig.get_legend_handles_labels()\n",
        "    fig.legend(handles, ['Clase MITF-low', 'Clase immune', 'Instancias test'], \n",
        "               framealpha=0.3, scatterpoints=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbMp7y5c3v44"
      },
      "source": [
        "### **2.3 Características de las SVM**\n",
        "\n",
        "Del mismo modo que el resto de paradigmas de clasificación, las SVMs poseen distintas propiedades que les hacen preferibles en distintos escenarios, o que pueden suponer complicaciones para su correcto uso. \n",
        "\n",
        "En primer lugar, se enumeran las principales ventajas de SVM:\n",
        "\n",
        "- Eficaces en problemas con una *alta dimensionalidad*. Obtienen buenas soluciones incluso cuando el número de variables de entrada es alto. \n",
        "- Buen comportamiento en aquéllos casos en que el *número de variables es mayor que el número de instancias*. Esto resulta de vital importancia en problemas de carácter bioinformático.\n",
        "- *Eficiente en memoria*. El modelo se basa únicamente en almacenar los \"vectores soporte\" encontrados en entrenamiento.\n",
        "- *Versátil*. Se adapta bien a distintos problemas configurando la función kernel más apropiada en cada caso de estudio.\n",
        "\n",
        "En segundo lugar, entre las desventajas de las SVM se incluyen:\n",
        "\n",
        "- *Dificultad en la parametrización*. El comportamiento del modelo SVM es muy dependiente de los parámetros seleccionados para el *coste*  y la función *kernel*. No es sencillo encontrar unos valores óptimos, y por tanto supone emplear una etapa denominada como \"hiperparametrización\" que tampoco garantiza los mejores resultados. \n",
        "- *Tratamiento de datos nominales*. Igual que el resto de clasificadores lineales, no están adaptados a características que no sean numéricas. La solución es aplicar una transformación que genere nuevas variables mediante una codificación binaria. \n",
        "- *Exclusividad en problemas binarios*. Por defecto, las SVMs no permiten una clasificación en problemas de más de dos clases, debiendo recurrir a  estrategias tipo \"divide-y-vencerás\" (transformar el problema en múltiple subproblemas de clases binarias). \n",
        "- *Cálculo poco preciso sobre las probabilidades de salida*. En caso que se necesite obtener valores de probabilidad o confianza para la salida, por ejemplo para calcular la métrica AUC, los valores que obtiene una SVM suelen ser poco fiables. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZHExvcnild"
      },
      "source": [
        "## **3. ENSEMBLES Y RANDOM FOREST**\n",
        "\n",
        "Los algoritmos posiblemente más utilizados en Ciencia de Datos son aquéllos basados en múltiples clasificadores, generalmente conocidos como \"ensembles\". \n",
        "\n",
        "En esta sección, se introduce en qué consiste esta tipología de sistemas de aprendizaje. Posteriormente, el foco se centra en uno de las aproximaciones más conocidas de este tipo de métodos: el algoritmo Random Forest. Por último, se indican algunas de las ventajas e inconvenientes relativos a este paradigma. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRYxY8SgcOb7"
      },
      "source": [
        "## **3.1 Introducción al paradigma de ensembles**\n",
        "\n",
        "Hasta el momento, se ha construido un modelo único a partir de los datos del problema, y éste ha permitido realizar una predicción más o menos acertada con respecto al conjunto de test. No obstante, en la actividad cotidiana es frecuente contar con el criterio de distintas fuentes a la hora de tomar decisiones. Agregar o combinar las \"opiniones\" de un grupo de expertos parece ser la clave para incrementar la confianza en una predicción concreta. \n",
        "\n",
        "Para que el anterior escenario sea realmente válido, existen dos premisas muy importantes:\n",
        "\n",
        "- Las decisiones/predicciones deben ser realizadas a partir de fuentes o expertos que tengan cierta credibilidad. De esta forma, se garantiza una calidad adecuada en la respuesta.\n",
        "- El grupo de expertos o colección de fuentes debería ser diversa entre sí. Así, se promueve cierto nivel de objetividad.\n",
        "\n",
        "En definitiva, en caso de poder contar con  la opinión de especialistas que presenten puntos de vista complementarios, ayudará a aumentar la calidad y fiabilidad de la decisión tomada.\n",
        "\n",
        "Esta premisa también tiene su aplicación directa en el Machine Learning, tomando la denominación de \"*modelos tipo Ensemble*\". En esta familia de algoritmos, se construye un número relativamente grande de clasificadores, utilizando para ello dos enfoques: \n",
        "\n",
        "- **Bagging**: Usar un subconjunto diferente de los datos de entrenamiento. En este caso, se entrenan `M` clasificadores de manera independiente. \n",
        "- **Boosting**: Usar pesos o costes para los ejemplos más difíciles de identificar correctamente. En este caso, se realizan `M` iteraciones, en cada una generando un clasificador dependiente del resultado de la etapa anterior.\n",
        "\n",
        "Este apartado del curso se centra exclusivamente en la primera de las estrategias, es decir, **bagging**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afzl9Aws_Cfd"
      },
      "source": [
        "### **3.2 Random Forest: Fundamentos e implementación en Python**\n",
        "\n",
        "Para comprender correctamente en qué se fundamenta el algoritmo Random Forest, hay que definir con detalle la metodología de **bagging**. Si se tiene un conjunto de datos $X$ de tamaño $N$, se podría  generar un nuevo conjunto tomando $N$ instancias de manera aleatoria, con **reemplazamiento**. Si este procedimiento se repite $M$ veces, se obtiene un número suficiente de conjuntos para que su \"combinación\" represente la distribución original de los datos. \n",
        "\n",
        "En el caso de la tarea de aprendizaje supervisado, se crearán $M$ nuevos conjuntos de entrenamiento, todos del tamaño original, pero con algunas de las instancias repetidas en cada nuevo conjunto. De este modo, se pueden aprender $M$ clasificadores independientes, cada uno especializado en las instancias que usadas en su entrenamiento. \n",
        "\n",
        "Al ser cada modelo relativamente independiente entre sí, y generando cada uno un valor de salida que puede diferir, la pregunta principal que surge en este momento es ¿cómo se unifica la respuesta del modelo global? En el caso de *clasificación* la más común es utilizar un voto por mayoría.\n",
        "\n",
        "*Random Forest* es un algoritmo de aprendizaje utilizado indistintamente para clasificación o regresión, y que se basa en el anterior principio de **bagging**. En este caso, el nombre *forest* (bosque en inglés) permite deducir cuál es la familia de modelos que se entrenan en cada submuestra del conjunto de entrenamiento: los **árboles de decisión**.\n",
        "\n",
        "Pero este algoritmo añade una nueva componente a la metodología **bagging**, y es la de realizar también una selección de variables de entrada en cada subconjunto de entrenamiento. De esta forma, se obtiene una doble ganancia. Por un lado, se incrementan las diferencias entre cada conjunto, y por tanto entre cada modelo generado. Por otro, el tiempo de aprendizaje y la complejidad de cada modelo se ve reducida. \n",
        "\n",
        "![Ejemplo de funcionamiento de Random Forest](https://i.imgur.com/QhZMpbv.png)\n",
        "\n",
        "Retomando las dos características principales a cumplir para la metodología tipo Ensemble, en Random Forest se obtiene lo siguiente:\n",
        "\n",
        "- Los clasificadores utilizados se basan en árboles de decisión, y por tanto resultan muy apropiados a la hora de realizar el aprendizaje.\n",
        "- La selección de instancias, junto con la selección de variables, permite que la construcción de cada árbol individual sea relativamente distinto a la de los demás modelos, donde la unión hace la fuerza.\n",
        "\n",
        "Utilizar el algoritmo Random Forest en *Scikit-Learn* es tan sencillo como cualquier otro clasificador, como se muestra el siguiente ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99eEaW0unv3X"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train) \n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "print(\"El porcentaje de acierto obtenido por RF es\",acc*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHMP02vml8K6"
      },
      "source": [
        "metrics.plot_confusion_matrix(rf, X_test, y_test,cmap='binary')\n",
        "plt.title(\"Matriz de confusión obtenida para el clasificador RF\")\n",
        "plt.show()\n",
        "\n",
        "print(metrics.classification_report(y_test,y_pred))\n",
        "\n",
        "f1 = metrics.f1_score(y_test,y_pred,pos_label=\"immune\")\n",
        "print(\"La medida F1 para el clasificador %s es %.4f\"%(rf.__class__.__name__,f1))\n",
        "\n",
        "y_probs = rf.predict_proba(X_test)\n",
        "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
        "print(\"La medida AUC para el clasificador %s es %.4f\"%(rf.__class__.__name__,auc))\n",
        "metrics.plot_roc_curve(rf, X_test, y_test)\n",
        "plt.title(\"Curva ROC obtenida para el clasificador RF\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hhEZPyJyhIi"
      },
      "source": [
        "La implementación de *Scikit-Learn* de Random Forest tiene una doble ventaja muy interesante. \n",
        "\n",
        "1. Por un lado, permite identificar cuáles son las variables de entrada más importantes utilizados para predecir la salida. Para ello, se debe consultar una de las propiedades del modelo creado, ``feature_importances_``, que determina el peso de cada variable, normalizado a suma 1. \n",
        "\n",
        "2. Por otro lado, se puede transformar el conjunto de datos de entrada de acuerdo a la información anterior, para de este modo simplificar y mejorar un posterior proceso de aprendizaje. En concreto, el método  ``transform()`` reducirá el dataset dejando sólo las variables seleccionadas. \n",
        "\n",
        "A continuación se muestra un ejemplo de cómo llevar a cabo este proceso:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUs5qXrONtUU"
      },
      "source": [
        "#Para realizar selección de características, usamos lo siguiente:\n",
        "from sklearn.feature_selection import SelectFromModel \n",
        "\n",
        "#En primer lugar, se capturan los valores del ranking de importancia\n",
        "importancia = rf.feature_importances_\n",
        "#Se representan las 5 más importantes\n",
        "(pd.Series(importancia, index=X_train.columns).nlargest(5).plot(kind='barh'))\n",
        "plt.show()\n",
        "\n",
        "#A continuación, se transforma el conjunto de entrenamiento y test \n",
        "#Para ello, se utilizarán únicamente las variables más importantes según RF\n",
        "fs = SelectFromModel(rf,prefit=True)  #instrucción que define las variables\n",
        "X_train_fs = fs.transform(X_train)    #Transformación del conjunto de train\n",
        "X_test_fs = fs.transform(X_test)      #Transformación del conjunto de test\n",
        "\n",
        "rf_fs = RandomForestClassifier(random_state=42)      #Reentrenamos modelo específico de menos variables\n",
        "rf_fs.fit(X_train_fs, y_train)\n",
        "\n",
        "y_pred = rf_fs.predict(X_test_fs)\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "print()\n",
        "print(\"El acierto con el modelo simplificado a %d variables es %.4f\"%(X_test_fs.shape[1],acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiVnINDquWP_"
      },
      "source": [
        "El algoritmo Random Forest presenta tres parámetros principales:\n",
        "\n",
        "- `n_estimators`: un valor entero (`int`) que indica el número de árboles de decisión utilizados (por defecto=100).\n",
        "- `max_depth`: un valor entero (`int`) que establece máxima profundidad de cada árbol. Si no hay ninguna, entonces los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de `min_samples_split` instancias (por defecto=`None`).\n",
        "- `max_features`: un valor a elegir entre {\"auto\", \"sqrt\", \"log2\"}, o bien un valor entero o real (`int` o `float`). Se refiere al número de variables a considerar cuando se busca la mejor división (por defecto=`\"auto\"`, que equivale a `sqrt` o raiz cuadrada del número de variables). \n",
        "\n",
        "En general, la preferencia es tener un alto número de árboles (entre 100 y 500) con una gran profundidad (dejar como `None`), dejando también por defecto el número de variables. Sin embargo, hay que considerar que incrementar los dos primeros parámetros implicará un mayor coste computacional, alargando quizá inncesariamente el tiempo de entrenamiento. Es siempre conveniente validar y ajustar los valores de los parámetros de manera independiente para cada caso de estudio. \n",
        "\n",
        "En el siguiente ejemplo, se muestra la frontera de decisión obtenida por Random Forest con los parámetros por defecto determinados por *Scikit-Learn*. Hay que tener en cuenta que en este problema transformado a dos únicas variables, la potencia de Random Forest con respecto a la diversidad se pierde, y la potencia predictiva tenderá a reducirse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml3QYHvdsZcf"
      },
      "source": [
        "#Creamos y entrenamos el clasificador con los datos 2D\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_2D_train, y_2D_train)\n",
        "score = clf.score(X_2D_test,y_2D_test)\n",
        "\n",
        "fig = plt.figure(figsize=(12,9))\n",
        "fig = plot_decision_regions(clf=clf,X=X_2D,y=y_int.to_numpy().ravel(),\n",
        "                            X_highlight=X_2D_test, legend=2,\n",
        "                            scatter_kwargs=scatter_kwargs,\n",
        "                            contourf_kwargs=contourf_kwargs,\n",
        "                            scatter_highlight_kwargs=scatter_highlight_kwargs)\n",
        "plt.title('Frontera de decisión generada por el clasificador '+clf.__class__.__name__)\n",
        "plt.text(4 - .3, -3 + .3, ('Acc tst: %.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
        "handles, labels = fig.get_legend_handles_labels()\n",
        "fig.legend(handles, ['Clase MITF-low', 'Clase immune', 'Instancias test'], \n",
        "           framealpha=0.3, scatterpoints=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI_Sksm-yvRi"
      },
      "source": [
        "En el siguiente trozo de código, puede observarse la estructura de uno de los árboles contenidos dentro de RandomForest:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqee0sHey1eN"
      },
      "source": [
        "#Bibliotecas necesarias para una mejor visualización\n",
        "from sklearn import tree\n",
        "from graphviz import Source\n",
        "\n",
        "rf_full = RandomForestClassifier()\n",
        "rf_full.fit(X, y)\n",
        "dt = rf_full.estimators_[0]\n",
        "\n",
        "#se pinta el árbol:\n",
        "tree_graph = tree.export_graphviz(dt, out_file=None, \n",
        "                                  feature_names=X.columns, \n",
        "                                  class_names=pd.unique(y[y.columns[0]]), \n",
        "                                  filled = True)\n",
        "graph = Source(tree_graph)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCI66qY8_POM"
      },
      "source": [
        "### **3.3 Características de Random Forest**\n",
        "\n",
        "Tal como se indicó al comienzo de esta sección, el algoritmo Random Forest es quizá uno de los más utilizados en tareas de clasificación, tanto por usuarios expertos como por aquéllos que se inician en el análisis de datos. Los principales motivos se asocian a las siguientes características: \n",
        "\n",
        "- Robusto frente al *overfitting*, en contraposición con el comportamiento de los árboles de decisión individuales. \n",
        "- La configuración de sus parámetros es bastante simple e intuitiva, y su habilidad predictiva es buena incluso utilizando los valores por defecto.\n",
        "- Funciona muy bien cuando el número de variables de entrada del problema es grande y para una gran cantidad de datos.\n",
        "- Permite realizar una selección de variables de alta calidad.\n",
        "\n",
        "Sin embargo, existen algunos aspectos también negativos relativos al método Random Forest a tener en cuenta:\n",
        "\n",
        "- El aprendizaje puede ser lento dependiendo de la parametrización, es decir, número de árboles y profundidad.\n",
        "- En contraposición con los árboles de decisión simples, el modelo global Random Forest no resulta directamente interpretable por el usuario. La razón es evidente, y es que el clasificador global tiene un elevado número de árboles, que habría que revisar individualmente.\n",
        "- No capta las posibles correlaciones entre las variables de entrada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyf5ow0g1LRo"
      },
      "source": [
        "## **REFERENCIAS BIBLIOGRÁFICAS**\n",
        "\n",
        "-\tHan, J., Kamber, M., Pei, J. (2011). Data Mining: Concepts and Techniques. San Francisco, CA, USA: Morgan Kaufmann Publishers. ISBN: 0123814790, 9780123814791\n",
        "- Scikit-Learn: Supervised Learning https://scikit-learn.org/stable/supervised_learning.html (visitado el 25 de Junio de 2020).\n",
        "- Open Machine Learning Course: Topic 5. Ensembles of algorithms and random forest. Part 1. Bagging https://mlcourse.ai/articles/topic5-part1-bagging/ (visitado el 25 de Junio de 2020).\n",
        "- Open Machine Learning Course: Topic 5. Ensembles of algorithms and random forest. Part 2. Random Forest https://mlcourse.ai/articles/topic5-part2-rf/ (visitado el 25 de Junio de 2020).\n",
        "- Open Machine Learning Course: Topic 5. Ensembles of algorithms and random forest. Part 3. Feature Importances https://mlcourse.ai/articles/topic5-part3-feature-importance/ (visitado el 25 de Junio de 2020).\n",
        "\n",
        "### **Referencias adicionales**\n",
        "\n",
        "-\tAlpaydin, E. (2016). Machine Learning: The New AI. MIT Press. ISBN: 9780262529518\n",
        "-\tWitten, I. H., Frank, E., Hall, M. A., Pal, C. J. (2017). Data mining: practical machine learning tools and techniques. Amsterdam; London: Morgan Kaufmann. ISBN: 9780128042915 0128042915\n",
        "- Towards Data Science: Support Vector Machines(SVM) — An Overview https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989 (visitado el 25 de Junio de 2020). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCoeT081CESG"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "MOOC Machine Learning y Big Data para la Bioinformática (1ª edición)   \n",
        "\n",
        "http://abierta.ugr.es     \n",
        "\n",
        "![CC](https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-nd.png)    \n",
        "</div>    "
      ]
    }
  ]
}