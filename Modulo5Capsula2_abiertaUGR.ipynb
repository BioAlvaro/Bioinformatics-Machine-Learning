{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Modulo5Capsula2_abiertaUGR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vD-6nUjHFjP"
      },
      "source": [
        "![cabecera_slide_abiertaugr_bigdata.jpg](https://i.imgur.com/HXn24wC.jpg)\n",
        "## Módulo 5.2 Métodos estándar en clasificación.\n",
        "\n",
        "**Autor**: \n",
        "\n",
        "*Por* Prof. Alberto Fernández Hilario\n",
        "\n",
        "Profesor Titular de Universidad de Granada. Instituto Andaluz Interuniversitario en Data Science and Computational Intelligence (DasCI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Y_zJaSHFjR"
      },
      "source": [
        "## Breves Instrucciones\n",
        "\n",
        "### Recordatorio: Introducción a NoteBook\n",
        "\n",
        "El cuaderno de *Jupyter* (Python) es un enfoque que combina bloques de texto (como éste) junto con bloques o celdas de código. La gran ventaja de este tipo de celdas, es su interactividad, ya que pueden ser ejecutadas para comprobar los resultados directamente sobre las mismas. \n",
        "\n",
        "**Muy importante**: el orden de las instrucciones (bloques de código) es fundamental, por lo que cada celda de este cuaderno debe ser ejecutada secuencialmente. En caso de omitir alguna, puede que el programa lance un error (se mostrará un bloque salida con un mensaje en inglés de color rojo), así que se deberá comenzar desde el principio en caso de duda. Para hacer este paso más sencillo, se puede acceder al menú “Entorno de Ejecución” y pulsar sobre “Ejecutar anteriores”. \n",
        "\n",
        "¡Ánimo!\n",
        "\n",
        "Haga clic en el botón \"play\" en la parte izquierda de cada celda de código. Las líneas que comienzan con un hashtag (#) son comentarios y no afectan a la ejecución del programa.\n",
        "\n",
        "También puede pinchar sobre cada celda y hacer \"*ctrl+enter*\" (*cmd+enter* en Mac).\n",
        "\n",
        "Cuando se ejecute el primero de los bloques, aparecerá el siguiente mensaje: \n",
        "\n",
        "\"*Advertencia: Este cuaderno no lo ha creado Google.*\n",
        "\n",
        "*El creador de este cuaderno es \\<autor\\>@go.ugr.es. Puede que solicite acceso a tus datos almacenados en Google o que lea datos y credenciales de otras sesiones. Revisa el código fuente antes de ejecutar este cuaderno. Si tienes alguna pregunta, ponte en contacto con el creador de este cuaderno enviando un correo electrónico a \\<autor\\>@go.ugr.es.”*\n",
        "\n",
        "No se preocupe, deberá confiar en el contenido del cuaderno (Notebook) y pulsar en \"*Ejecutar de todos modos*\". Todo el código se ejecuta en un servidor de cálculo externo y no afectará en absoluto a su equipo informático. No se pedirá ningún tipo de información o credencial, y por tanto podrá seguir con el curso de forma segura. \n",
        "\n",
        "Cada vez que ejecute un bloque, verá la salida justo debajo del mismo. La información suele ser siempre la relativa a la última instrucción, junto con todos los `print()` (orden para imprimir) que haya en el código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ngT7wvYzzo"
      },
      "source": [
        "## **ÍNDICE**\n",
        "\n",
        "En este *notebook*: \n",
        "1. Se introducirán los paradigmas de clasificación de \"caja blanca\".\n",
        "2. Se describirán los métodos de clasificación basados en modelos lineales.\n",
        "3. Se estudiará el algoritmo de clasificación basado en el vecino más cercano.\n",
        "4. Se presentará el algoritmo para obtener árboles de decisión para clasificación. \n",
        "5. Se analizarán las ventajas e inconvenientes de todos los clasificadores anteriores.\n",
        "6. Se mostrará cómo utilizar todos estos clasificadores desde Python con *Scikit-Learn*.\n",
        "7. Se mostrarán ejemplos gráficos de las fronteras de decisión de cada modelo, para comprender sus principales diferencias en cuanto a su funcionamiento. \n",
        "\n",
        "    \n",
        "Contenidos:\n",
        "1. Introducción   \n",
        "2. Modelos simples: regresión lineal y logística    \n",
        "3. Clasificación con el Vecino Más Cercano (kNN)   \n",
        "4. Árboles de decisión  \n",
        "5. Referencias bibliográficas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo3h6f7lH_Am"
      },
      "source": [
        "## **1. INTRODUCCIÓN**\n",
        "\n",
        "En esta primera sección, se realizará una introducción a los paradigmas de clasificación en general, y a los específicos que se describen en este apartado del curso. A continuación, se cargarán los datos del caso de estudio de cáncer de melanoma con los que se viene trabajando hasta ahora. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdkGqAgBpYs"
      },
      "source": [
        "### **1.1 Paradigmas de clasificación**\n",
        "\n",
        "En la cápsula 1 de este Módulo se destacó que existen diferentes tipos de funciones discriminantes. Esto permite enumerar a su vez distintos paradigmas o modelos de clasificación: \"*modelos de caja blanca*\" y \"*modelos de caja negra*\". Este apartado del curso se centrará principalmente en el primer tipo, ya que son la \"primera línea de ataque\" cuando se quiere resolver un problema de clasificación. \n",
        "\n",
        "La razón principal es que estos modelos se obtienen de forma relativamente rápida y simple, y son en general altamente interpretables. Esto significa que se pueden examinar las componentes del modelo e identificar cuáles son las variables clave utilizadas para realizar la división entre las clases. De este modo, permiten al usuario comprender si éstas tienen un sentido con respecto al problema que se está estudiando, por ejemplo desde el punto de vista biológico. \n",
        "\n",
        "Entre los diferentes algoritmos de clasificación destacados en este grupo, se introducirán los basados en regresión lineal y logística, el vecino más cercano, y los árboles de decisión. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GDXE_2kHFjS"
      },
      "source": [
        "### **1.2 Cargar los datos del problema**\n",
        "\n",
        "Con el objetivo de comprobar el comportamiento de los diferentes algoritmos de clasificación, se comienza incorporando los datos del caso de estudio que sirve como hilo conductor del presente curso. La notación o código utilizado es exactamente el mismo que en actividades anteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FFxCHjDHFjT"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Cargamos los datos ómicos de la matriz de expresión desde un fichero compartido en Google Drive\n",
        "gene_exp_inmune = pd.read_csv('https://drive.google.com/uc?id=1PYzEIdmnfjOnBpPDIFBE9hL1Lkj_OBCk',index_col=0)\n",
        "#Cargamos la variable clínica correspondiente a las etiquetas \"inmune\" vs. \"MITF-low\"\n",
        "clinical_info_inmune = pd.read_csv('https://drive.google.com/uc?id=1hHQfcvrFa5Jds-9tW_X4sHjKpYKdii9s',index_col=0)\n",
        "\n",
        "X, y = gene_exp_inmune, clinical_info_inmune\n",
        "\n",
        "#Imprimimos las 5 primeras muestras del conjunto de datos para comprobar que se ha cargado correctamente\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOhFznv1pXKR"
      },
      "source": [
        "Adicionalmente, se va a proceder a transformar el conjunto de datos a solamente dos dimensiones (dos variable de entrada) utilizando las componentes principales (PCA) que ya se introdujo en el Módulo 2. De este modo, se podrá representar de manera muy sencilla las fronteras de clasificación obtenidas por cada técnicas de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVBRTxy_80A4"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Transformamos el conjunto de datos inicial para que esté representado por solo 2 variables\n",
        "n_componentes = 2\n",
        "pca = PCA(n_components=n_componentes)\n",
        "X_2D = pca.fit_transform(X)\n",
        "  \n",
        "#Se transforma el rango de cada variable a [0, 1]\n",
        "st = StandardScaler()\n",
        "X_2D = st.fit_transform(X_2D)\n",
        "\n",
        "#Pintamos en un gráfico de puntos (scatterplot) el nuevo conjunto 2D\n",
        "plt.scatter(X_2D[:, 0], X_2D[:, 1], cmap=plt.cm.Set1, c=pd.get_dummies(y).iloc[:,0], edgecolor='k')\n",
        "plt.title(\"Representación 2D del problema de cáncer de melanoma\")\n",
        "plt.xlabel('PCA_V1')\n",
        "plt.ylabel('PCA_V2')\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mRNTpBb3KqX"
      },
      "source": [
        "Por simplicidad, para los ejemplos incluidos en este NoteBook se utilizará una validación tipo \"*hold-out*\" por defecto. Para más detalles consúltese el Módulo 3 sobre Aprendizaje Supervisado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKBbSjLb3JpA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "y_int = pd.get_dummies(y).iloc[:,0]\n",
        "X_2D_train, X_2D_test, y_2D_train, y_2D_test = train_test_split(X_2D, y_int, random_state=42)\n",
        "\n",
        "print(\"Numero de instancias en entrenamiento: {}; y test: {}\".format(len(X_train),len(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBSvtYTs2vQz"
      },
      "source": [
        "## **2. MODELOS SIMPLES: REGRESIÓN LINEAL Y LOGÍSTICA**\n",
        "\n",
        "En el Módulo 4 sobre **Aprendizaje Supervisado: Regresión** se estudió que se pueden construir funciones que aproximen los valores de salida reales para un conjunto de datos. Esta misma idea se puede aplicar directamente a clasificación, tomando en consideración que ahora los valores de salida no están en un rango real, si no que pertenecen a un conjunto *n-ario* (binario para dos clases). \n",
        "\n",
        "En primer lugar, se introducirá el modelo más sencillo posible basado en una regresión lineal simple, es decir, un hiperplano de separación (una \"recta\" que divide las instancias). A continuación, extenderemos esta idea hacia la denominada como *regresión logística*. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw77cYt5BwqI"
      },
      "source": [
        "### **2.1 Modelo de regresión lineal**\n",
        "\n",
        "Se describirán las características principales de este modelo, y cómo se puede utilizar mediante Scikit-Learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWPWmgO6B6wf"
      },
      "source": [
        "#### 2.1.1. Introducción al modelo lineal\n",
        "\n",
        "De acuerdo a lo comentado al comienzo de esta sección, para adaptar el formato de aprendizaje de *regresión lineal* a clasificación, bastará con aprender o ajustar los coeficientes de un hiperplano intentando aproximar la salida a los valores por defecto {0, 1}, siguiendo el mismo esquema de minimizar la suma residual de cuadrados. En este caso, se está teniendo en cuenta que se trabaja con un problema de clasificación binario, donde la primera clase se identificará con el valor \"0\", mientras que la segunda tendrá el valor \"1\". \n",
        "\n",
        "Antes de continuar, destacar que los hiperplanos son simples \"cortes con una recta\" para la clasificación de las instancias, es decir, una frontera de decisión. De esta forma, las instancias (o puntos de datos) que se encuentren a cada lado del hiperplano (\"recta\")  serán predichas como una clase distinta. Como es lógico, la dimensión del hiperplano depende del número de variables de entrada. Si éste es 2, entonces el hiperplano es una sencilla línea recta. Cuando el número de variables es 3, entonces el hiperplano se convierte en un plano bidimensional. Manejar más de 3 dimensiones se hace complicado para un usuario humano.\n",
        "\n",
        "La fórmula obtenida que divide el espacio de entrada en dos partes será la siguiente:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}(x,w) = w_0 + w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n\n",
        "\\end{equation}\n",
        "\n",
        "donde $w = (w_1, \\ldots, w_n)$ serán los coeficientes asociados a cada variable de entrada, y $w_0$ el término independiente. En el caso trivial para un problema de una única dimensión, (una variable $x$), se buscaría la clásica recta $y = a \\cdot x+b$.\n",
        "\n",
        "![Ejemplo de regresión lineal para clasificación](https://i.imgur.com/OaGZIa8.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oImqO4aIB8cw"
      },
      "source": [
        "#### 2.1.2 Implementación en Scikit-Learn\n",
        "\n",
        "Para aplicar la regresión lineal en tareas de clasificación, la sintaxis de *Scikit-Learn* es equivalente al caso de regresión. En este caso de ejemplo, se muestran los coeficientes calculados para cada variable, almacenados en la variable miembro `coef_`, y el término independiente en ```intercept_```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfG095Ug2_T8"
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "lm = linear_model.LinearRegression()\n",
        "\n",
        "#Aquí hacemos un pequeño \"truco\" que es transformar la salida categórica a valores {0,1}\n",
        "#Este paso es necsario para aplicar el modelo de \"regresión\" que espera una salida de tipo real.\n",
        "y_train_int = pd.get_dummies(y_train).iloc[:,0]\n",
        "\n",
        "lm.fit(X_train, y_train_int)\n",
        "print(lm.intercept_)\n",
        "lm.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZydKV7vY4vkZ"
      },
      "source": [
        "Del mismo modo que sucedía en las tareas de regresión (**Módulo 4**), el valor absoluto de cada coeficiente indica la importancia de dicha variable en la aproximación de la salida. De este modo, podemos interpretar cuáles son las propiedades de los datos que mayor influencia tienen en la distinción de las clases.\n",
        "\n",
        "En el siguiente cuadro, se muestra un gráfico resumen de la importancia de las variables (subconjunto del perfil genético de los 5 más importantes) en base al valor de los coeficientes de la función discriminante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zR56hRe3sir"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "importancia = np.abs(lm.coef_) #transformamos a una lista uni-dimensional\n",
        "#Normalizamos los valores:\n",
        "importancia_norm = normalize(importancia[:,np.newaxis], axis=0).ravel()\n",
        "\n",
        "#Se representan las 5 más importantes según valor absoluto\n",
        "(pd.Series(importancia_norm, index=X_train.columns).nlargest(5).plot(kind='barh'))\n",
        "plt.title(\"Ratio de importancia del panel genético según Linear Regressor\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdMsq4PT3ejr"
      },
      "source": [
        "A continuación, se examinará la calidad de predicciones generadas de acuerdo a la medida estándar de accuracy (porcentaje de acierto). En primer lugar, observamos que la salida obtenida por el modelo no es un valor discreto, si no que es un valor real dado que estamos usando una fórmula de regresión. \n",
        "\n",
        "No obstante, como se indicó anteriormente se va a considerar cada clase como un valor entero. En este caso de estudio de clasificación \n",
        "binaria, la clase \"*MITF-Low*\" será clase 0, mientras que \"*immune*\" será clase 1. Así, se procederá con un umbral de corte igual a 0.5 para determinar la clase final. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4EaikpSBTrx"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Volvemos a transformar la salida a un valor {0,1}\n",
        "y_test_int = pd.get_dummies(y_test).iloc[:,0]\n",
        "y_pred = lm.predict(X_test)\n",
        "\n",
        "print(\"Valores de predicción originales (10 primeros):\")\n",
        "print(y_pred[:10])\n",
        "print()\n",
        "\n",
        "print(\"Valores de predicción redondeados (10 primeros):\")\n",
        "\n",
        "y_pred[y_pred >= 0.5] = 1\n",
        "y_pred[y_pred < 0.5] = 0\n",
        "\n",
        "print(y_pred[:10])\n",
        "print()\n",
        "\n",
        "acc_score = accuracy_score(y_test_int, y_pred)\n",
        "print(\"Accuracy obtenido:\",acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vp3OxUhkspI"
      },
      "source": [
        "### **2.2 Modelo de Regresión Logística**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_qG8MppDga0"
      },
      "source": [
        "#### 2.2.1 Introducción a la regresión logística\n",
        "\n",
        "A pesar de los buenos resultados mostrados, la regresión lineal resulta limitada en muchos casos. Por este motivo, en las tareas de Ciencia de Datos se prefiere utilizar una técnica conocida como *regresión logística*. A pesar de su nombre, es un modelo lineal de clasificación más que de regresión. \n",
        "\n",
        "En este caso, la función de salida que se desea aproximar sería la siguiente:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}(w,x) = \\frac{1}{1+e^{ w_0 + w_1 \\cdot x_1 + \\ldots + w_n \\cdot x_n}}\n",
        "\\end{equation}\n",
        "\n",
        "La ventaja de esta aproximación, sobre la regresión lineal, es que crea una separación más suave entre los valores de la variable de salida (clases):\n",
        "\n",
        "![Comparativa Regresión Logística vs. Regresión Lineal](https://i.imgur.com/XTKnKp4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0d48MKfDjqZ"
      },
      "source": [
        "#### 2.2.2 Implementación en Scikit-Learn y principales parámetros\n",
        "\n",
        "A continuación, se indican los pasos para construir un clasificador de este tipo en Scikit-Learn. Obsérvese que se sigue exactamente el mismo esquema que el visto anteriormente para `LinearRegressor`; sin embargo, en este caso al tratarse de un clasificador, la salida sí está en el conjunto `{0, 1}`. \n",
        "\n",
        "Existen diferentes parámetros que se pueden configurar, aunque se recomienda dejar los valores por defecto. En cualquier caso, a continuación se enumeran los más importantes:\n",
        "\n",
        "- `penalty` un valor a elegir entre `{'l1', 'l2', 'elasticnet', 'none'}`. Se usa para especificar la norma usada en la función de penalización para ajustar los coeficientes (por defecto = `'l2'`)\n",
        "\n",
        "- `C` un valor real (`float`) para forzar mayor o menor sobreaprendizaje, valores altos implican un mejor ajuste en entrenamiento (por defecto = `1.0`)\n",
        "\n",
        "- `class_weight` a elegir entre `{'balanced', None}` para dar o no mayor importancia a las muestras de clases menos representadas (por defecto = `None`)\n",
        "\n",
        "Aunque no se indique explícitamente, muchos de los clasificadores implementados en *Scikit-Learn* poseen por defecto el parámetro `class_weight` con el que se puede abordar el problema del desequilibrio de clases (*imbalanced classification*).\n",
        "\n",
        "En el siguiente bloque de código, se repiten los mismos pasos realizados para `Linear Regression` pero con el clasificador `Logistic Regressor`. Obsérvese que el mismo grupo de variables (genes) se destacan como principales en ambos casos. Finalmente, el acierto obtenido por este segundo modelo es más alto, indicando la preferencia por este tipo de soluciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3qI5KsxoIpQ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #ignorar esta linea\n",
        "\n",
        "lrm = linear_model.LogisticRegression()\n",
        "\n",
        "lrm.fit(X_train, y_train.to_numpy().ravel())\n",
        "print(\"Coef. independiente:\",lrm.intercept_)\n",
        "print(\"Coef. por variable:\",lrm.coef_)\n",
        "print()\n",
        "\n",
        "importancia = np.abs(lrm.coef_[0]) #transformamos a una lista uni-dimensional\n",
        "importancia_norm = normalize(importancia[:,np.newaxis], axis=0).ravel()\n",
        "#Se representan las 5 más importantes según valor absoluto\n",
        "(pd.Series(importancia, index=X_train.columns).nlargest(5).plot(kind='barh'))\n",
        "plt.title(\"Ratio de importancia del panel genético según Logistic Regressor\")\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "y_pred = lrm.predict(X_test)\n",
        "\n",
        "print(\"Valores de predicción originales (10 primeros):\")\n",
        "print(y_pred[:10])\n",
        "print()\n",
        "\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "print(\"Acierto de Logistic Regression en la partición de test:\", acc_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtLVDHb4ZaHl"
      },
      "source": [
        "Además de la medida clásica de *accuracy*, se pueden utilizar otras diferentes como la medida *F1* que compensa entre los aciertos de cada clase, o la medida AUC. Utilizando un estimador o clasificador de *Scikit-Learn* es inmediato realizar la representación gráfica.\n",
        "\n",
        "En el siguiente trozo de código, se repasan varios modos de obtener las métricas antes mencionadas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbHZMj9rZw1Q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "metrics.plot_confusion_matrix(lrm, X_test, y_test,cmap='binary')\n",
        "plt.title(\"Matriz de confusión obtenida para el clasificador Logistic Regressor\")\n",
        "plt.show()\n",
        "\n",
        "print(metrics.classification_report(y_test,y_pred))\n",
        "\n",
        "f1 = metrics.f1_score(y_test,y_pred,pos_label=\"immune\")\n",
        "print(\"La medida F1 para el clasificador %s es %.4f\"%(lrm.__class__.__name__,f1))\n",
        "\n",
        "y_probs = lrm.predict_proba(X_test)\n",
        "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
        "print(\"La medida AUC para el clasificador %s es %.4f\"%(lrm.__class__.__name__,auc))\n",
        "metrics.plot_roc_curve(lrm, X_test, y_test)\n",
        "plt.title(\"Curva ROC obtenida para el clasificador Logistic Regressor\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnC1_xm7BUE3"
      },
      "source": [
        "En lo sucesivo, se va a representar la frontera de decisión obtenida por cada clasificador empleado.  Lo más importante con respecto a la representación de la función de decisión o discriminante, es que facilitan la comprensión sobre cómo funcionan realmente los distintos paradigmas de clasificación. \n",
        "\n",
        "Es muy probable que no coincida exactamente el `accuracy` obtenido con respecto al conjunto de datos original, pero es entendible puesto que hemos transformado el problema. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtqlD_4pp6WT"
      },
      "source": [
        "#Se importa una biblioteca especial para pintar en 2D\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "#Creamos y entrenamos el clasificador con los datos 2D\n",
        "clf = linear_model.LogisticRegression()\n",
        "clf.fit(X_2D_train, y_2D_train)\n",
        "score = clf.score(X_2D_test,y_2D_test)\n",
        "\n",
        "#Parámetros que se utilizarán para visualizar la figura\n",
        "scatter_kwargs = {'s': 120, 'edgecolor': None, 'alpha': 0.7}\n",
        "contourf_kwargs = {'alpha': 0.2}\n",
        "scatter_highlight_kwargs = {'s': 120, 'label': 'Test data', 'alpha': 0.7}\n",
        "\n",
        "fig = plt.figure(figsize=(12,9))\n",
        "fig = plot_decision_regions(clf=clf,X=X_2D,y=y_int.to_numpy().ravel(),\n",
        "                            X_highlight=X_2D_test, legend=2,\n",
        "                            scatter_kwargs=scatter_kwargs,\n",
        "                            contourf_kwargs=contourf_kwargs,\n",
        "                            scatter_highlight_kwargs=scatter_highlight_kwargs)\n",
        "plt.title('Frontera de decisión generada por el clasificador '+clf.__class__.__name__)\n",
        "plt.text(4 - .3, -3 + .3, ('Acc tst: %.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
        "handles, labels = fig.get_legend_handles_labels()\n",
        "fig.legend(handles, ['Clase MITF-low', 'Clase immune', 'Instancias test'], \n",
        "           framealpha=0.3, scatterpoints=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHZEXhPV3vpM"
      },
      "source": [
        "### **2.3 Ventajas e inconvenientes de los modelos lineales**\n",
        "\n",
        "Los modelos lineales poseen una serie de ventajas de gran interés para su uso práctico:\n",
        "\n",
        "- Son muy eficientes en su uso y por tanto resultan muy apropiados para una aproximación inicial al problema. \n",
        "- Una ventaja adicional es que son altamente interpretables, en el sentido que permiten determinar el peso o importancia de las variables asociadas a la clasificación. De este modo, se puede comprobar si los variables de entrada contienen un sentido biológico adecuado al caso de estudio. \n",
        "- Por último, suelen funcionar bien incluso en problemas con un alto número de variables, ya que en ambos modelos presentados, las estimaciones de los coeficientes se basan en la independencia de las variables de entrada. \n",
        "\n",
        "Debido al anterior motivo, hay que tener presente que en caso que estas variables de entrada estén correlacionados, la estimación resulta altamente sensible a los errores aleatorios en la variable de salida, produciendo una gran varianza. Por este motivo, si los datos del problema son relativamente complejos, es recomendable utilizar técnicas no lineales más sofisticadas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0MsaYJLiRmF"
      },
      "source": [
        "## **3. CLASIFICACIÓN CON EL VECINO MÁS CERCANO (KNN)**\n",
        "\n",
        "En la cápsula anterior de este módulo, se introdujo brevemente este algoritmo de clasificación debido a su sencillez. A pesar de ello, su aplicación resulta generalizada en muchos problemas de Ciencia de Datos y Machine Learning. \n",
        "\n",
        "En esta sección, se describe con más detalle el funcionamiento del algoritmo de vecino más cercano, para posteriormente conocer su uso mediante la implementación de Python en *Scikit-Learn*. Por último, se enumeran una serie de ventajas y desventajas relacionadas con esta técnica de clasificación. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmmZFW5NFBOa"
      },
      "source": [
        "### **3.1 Funcionamiento del algoritmo de vecino más cercano.**\n",
        "\n",
        "El algoritmo del vecino más cercano se encuadra en lo que se conoce como *Lazy Learning* o \"aprendizaje perezoso\" y es que realmente no existe fase de entrenamiento como tal. En lugar de ello, se realiza un \"aprendizaje basado en instancias\", es decir, el modelo es simplemente un almacen para las instancia de los datos de entrenamiento. \n",
        "\n",
        "La premisa para realizar la clasificación sobre una nueva instancia se basa en analizar la clase para instancia *similares*. En términos más sencillos, se diría que si \"camina como un pato, y grazna como un pato, seguramente sea un pato\". \n",
        "\n",
        "![Ejemplo de kNN para un pato](https://i.imgur.com/YgHbcZ6.png)\n",
        "\n",
        "La clave por tanto reside en qué se entiende por **similitud** entre instancias. Para ello, debemos definir lo que se conoce como función de distancia, que asignará un valor de salida entre instancias de acuerdo a cuán parecidos sean. Debido a que las instancias se representan en forma de variables numéricas, la más común de estas funciones de distancia será la **distancia euclídea**, que se describe mediante la siguiente ecuación:\n",
        "\n",
        "\\begin{equation}\n",
        "d_e(e_1, e_2) = \\sqrt{\\sum_{i=1}^{n}{(e_1^i - e_2^i)^2}}\n",
        "\\end{equation}\n",
        "\n",
        "Obsérvese que en el caso de dos dimensiones, coincidiría con la fórmula para hallar la longitud de la hipotenusa en un triángulo rectángulo, siendo los vértices de los catetos los puntos sobre los que desea encontrar su distancia. \n",
        "\n",
        "En el algoritmo de vecino más cercano, para determinar la clase de salida de una nueva instancia, se debe calcular el valor de distancia para **todas** las instancias disponibles en el conjunto entrenamiento. A continuación, se asigna la clase más frecuente de entre los **k** vecinos más cercanos a la nueva instancia. \n",
        "\n",
        "![Ejemplo de kNN](https://i.imgur.com/Z8sqWrw.gif)\n",
        "\n",
        "**K** es un parámetro crítico en el uso de este algoritmo de clasificación. Siempre debe ser un valor impar para evitar posibles empates. No obstante, la elección del valor óptimo de **k** depende en gran medida de los datos: en general, un valor mayor suprime los efectos del posible ruido, pero hace que los límites de la clasificación sean menos claros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxuIH8cStahI"
      },
      "source": [
        "### **3.2 Implementación en Scikit-Learn y principales parámetros de uso**\n",
        "\n",
        "Recuérdese que la implementación del vecino más cercano en *Scikit-learn* se encuentra en el método `KNeighborsClassifier` (del inglés *Clasificador de K Vecinos*). Los parámetros más importantes que se pueden configurar en este método son los siguientes:\n",
        "\n",
        "- `n_neighbors` un valor entero (`int`) para determinar el entorno del vecindario (por defecto = `5`)\n",
        "- `weights` a elegir entre `{'uniform', 'distance'}` según se quiera que la salida sea por voto simple, o la etiqueta de salida de los vecinos más cercanos tenga mayor importancia, respectivamente (por defecto = `'uniform'`)\n",
        "- `metric`: a elegir entre {'euclidean', 'manhattan', 'chebyshev', 'minkowski', 'mahalanobis'}, para determinar el cálculo de la distancia (por defecto `'minkowski'`)\n",
        "\n",
        "En el siguiente ejemplo se observa su funcionamiento, tomando *k* (parámetro `n_neighbors`) igual a 3, y el resto por defecto. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_bLdC0jyeb"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier # cargamos la función desde la biblioteca\n",
        "\n",
        "#Ejemplo de uso de kNN\n",
        "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean') # instanciamos el modelo\n",
        "knn.fit(X_train, y_train) \n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "print(\"Acierto de KNN en la partición de test:\", acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2O6NxaRgX7u"
      },
      "source": [
        "Se repite el modo de obtener otras métricas de calidad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4VVzTG5gash"
      },
      "source": [
        "metrics.plot_confusion_matrix(knn, X_test, y_test,cmap='binary')\n",
        "plt.title(\"Matriz de confusión obtenida para el clasificador k Nearest Neighbor\")\n",
        "plt.show()\n",
        "\n",
        "print(metrics.classification_report(y_test,y_pred))\n",
        "\n",
        "f1 = metrics.f1_score(y_test,y_pred,pos_label=\"immune\")\n",
        "print(\"La medida F1 para el clasificador %s es %.4f\"%(knn.__class__.__name__,f1))\n",
        "\n",
        "y_probs = knn.predict_proba(X_test)\n",
        "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
        "print(\"La medida AUC para el clasificador %s es %.4f\"%(knn.__class__.__name__,auc))\n",
        "metrics.plot_roc_curve(knn, X_test, y_test)\n",
        "plt.title(\"Curva ROC obtenida para el clasificador k Nearest Neighbor\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv-DHYOb1SBg"
      },
      "source": [
        "Nuevamente, podemos comprobar cómo se definen las fronteras de clasificación para este algoritmo en particular. Nótese el tipo de función discriminante no lineal (deja de ser una \"recta\"), y en muchas ocasiones además local (muy \"cerrado\") en áreas concretas del espacio de datos. A diferencia de los modelos lineales anteriores, **kNN** basa su funcionamiento en el entorno de los ejemplos. \n",
        "\n",
        "El tiempo cálculo para obtener la representación gráfica podría resultar bastante mayor en este caso particular, dadas las características de eficiencia del algoritmo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8tEvX31XkZ"
      },
      "source": [
        "#Creamos y entrenamos el clasificador con los datos 2D\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_2D_train, y_2D_train)\n",
        "score = clf.score(X_2D_test,y_2D_test)\n",
        "\n",
        "fig = plt.figure(figsize=(12,9))\n",
        "fig = plot_decision_regions(clf=clf,X=X_2D,y=y_int.to_numpy().ravel(),\n",
        "                            X_highlight=X_2D_test, legend=2,\n",
        "                            scatter_kwargs=scatter_kwargs,\n",
        "                            contourf_kwargs=contourf_kwargs,\n",
        "                            scatter_highlight_kwargs=scatter_highlight_kwargs)\n",
        "plt.title('Frontera de decisión generada por el clasificador '+clf.__class__.__name__)\n",
        "plt.text(4 - .3, -3 + .3, ('Acc tst: %.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
        "handles, labels = fig.get_legend_handles_labels()\n",
        "fig.legend(handles, ['Clase MITF-low', 'Clase immune', 'Instancias test'], \n",
        "           framealpha=0.3, scatterpoints=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cImNzkKOz8QT"
      },
      "source": [
        "### **3.3 Ventajas e inconvenientes del algoritmo kNN**\n",
        "\n",
        "Por último, se analizan las ventajas y desventajas de este algoritmo de clasificación, que deberán ser tenidas en cuenta de acuerdo al caso de estudio que se esté analizando. \n",
        "\n",
        "- **Ventajas**:\n",
        "  - Funciona bien incluso con instancias ruidosas (cuyo valor puede ser erróneo), utilizando como parámetro un valor de `k` moderado (`k`>1). Esto implica que en caso que haya datos anómalos (instancias que puedan contener valores erróneos), éstos no deberían afectar a la salida del clasificador, puesto que se compensaría con el resto de datos que sí son correctos.\n",
        "  - Bastante eficaz: utiliza funciones locales lineales para aproximarse a la función objetivo. La frontera es no lineal y depende del muestreo de datos que se haya realizado. \n",
        "  - Válido tanto para la clasificación como para la regresión. \n",
        "  - Se puede utilizar fácilmente con prototipos. Esto significa que, si en lugar de utilizar todo el conjunto de entrenamiento, se pudiese hacer una selección de las instancias más importantes, el algoritmo sería mucho más rápido, sin perder capacidad predictiva.\n",
        "  - Disponible en la mayoría de los paquetes de software. \n",
        "\n",
        "- **Desventajas**:\n",
        "  - Muy ineficiente en la memoria, ya que todo el conjunto de datos debe ser almacenado en el sistema. \n",
        "  - La complejidad computacional es $O(d \\cdot n^2)$ con $O(d)$ la complejidad de la distancia. Por tanto, a mayor número de instancias, más lenta resulta la predicción. \n",
        "  - La distancia entre vecinos puede estar dominada por variables de entrada irrelevantes. Si no se realiza una correcta selección de variables, puede afectar en un alto grado a la predicción.\n",
        "  - La distancia converge al mismo valor en una alta dimensionalidad. Esto significa que cuando se tiene un alto número de variables, la distancia o similaridad tiende a ser idéntica para cualquier instancia sobre la que se calcule. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u3CpKKUmWNA"
      },
      "source": [
        "## **4. ÁRBOLES DE DECISIÓN**\n",
        "\n",
        "Los árboles de decisión (en inglés *Decision Trees* o DT) son una técnica de aprendizaje supervisado utilizado tanto para clasificación y la regresión. \n",
        "\n",
        "En esta sección, primero se presentan las propiedades de las técnicas basadas en árboles de decisión. A continuación, se describen los pasos necesarios para utilizar la implementación disponible en la biblioteca Scikit-Learn. Por último, se enumeran una serie de ventajas e inconvenientes asociadas a los árboles de decisión en general. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK4-z9gTFEIO"
      },
      "source": [
        "### **4.1 Introducción a árboles de decisión en clasificación**\n",
        "\n",
        "Un modelo de este tipo se basa en reglas de decisión simples, o condiciones, con el formato \"*si-entonces-sino*\" (`IF-THEN-ELSE`), normalmente dicotómicas (grupos de dos). Existe un orden jerárquico en la aplicación de las reglas, que se van encadenando hasta dar la decisión final. Por este motivo, la forma habitual de estructurar el modelo es en forma de árbol, de ahí su nombre. Sus elementos son los siguientes:\n",
        "\n",
        "- Cada hoja es una categoría (clase) correspondientes a la salida.\n",
        "- Cada nodo (parte interna del árbol) especifica una prueba simple a realizar (regla sobre una única tupla \\<variable, valor\\>). \n",
        "- Los descendientes de cada nodo son los posibles resultados de la prueba del nodo\n",
        "\n",
        "![Ejemplo de árbol de decisión para clasificar especies animales](https://i.imgur.com/xeeSMzw.png)\n",
        "\n",
        "Durante el aprendizaje, se busca que cada nodo realice una división disjunta sobre el número de instancias de cada clase. Este criterio de división se calcula mediante dos posibles medidas: la ganancia de información (*entropía*) y la impureza (*índice Gini*). En definitiva, se busca la combinación \\<variable, valor\\> que minimiza o maximiza los valores anteriores (entropía o gini, respectivamente), lo que implica que todas las instancias de una clase se \"agrupan\" sobre la parte \"SI\" de la regla, mientras que el resto irán a la parte \"SINO\". \n",
        "\n",
        "De acuerdo a lo anterior, las variables de entrada más importantes aparecerán en la parte superior (raíz) del árbol (en el ejemplo de la imagen anterior sería el color). Esto es debido a que son las que se escogen inicialmente como más adecuadas para separar entre las clases del problema. \n",
        "\n",
        "El proceso de entrenamiento normalmente se realiza de manera recursiva: se comienza identificando la raíz, y se continua hasta llegar a cada una de las hojas del árbol. Se determina que se ha llegado a una hoja (y por tanto no proceden más divisiones) de acuerdo a dos posibles razones:\n",
        "\n",
        "- Se ha alcanzado un umbral de pureza del nodo, marcado por el valor mínimo o máximo de la medida de división (entropía o ganancia). \n",
        "- El árbol ha llegado a un límite de profundidad máxima marcada por el usuario. La profundidad se mide con el número de nodos desde la raíz hasta la hoja. \n",
        "\n",
        "Normalmente, cuanto más profundo es el árbol, más complejas son las reglas de decisión y más ajustado es el modelo. No obstante, se debe tener cuidado ya que este ajuste puede llevar al sobreaprendizaje. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh__etLk3VlB"
      },
      "source": [
        "### **4.2 Implementación en Scikit-Learn y principales parámetros de uso**\n",
        "\n",
        "La clase de Scikit-Learn que incluye árboles de decisión se llama `DecisionTreeClassifier`. Su uso es idéntico al resto de clasificadores  implementados en esta biblioteca, por lo que nos centraremos en sus principales parámetros adicionales:\n",
        "\n",
        "- `criterion` a elegir entre `{\"gini\", \"entropy\"}`, para determinar la función que mide la calidad de una división (por defecto=`\"gini\"`).\n",
        "- `max_depth` es un número entero (`int`) que determinar la máxima profundidad del árbol. Si no se indica, entonces los nodos se expanden hasta que todas las hojas sean puras o hasta que contengan menos de `min_samples_split` instancias.\n",
        "- `min_samples_split` es un número entero (`int`) que indica el número mínimo de instancias necesarias para dividir un nodo interno (por defecto=`2`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10nneINgm_g2"
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "print(\"Acierto de DT en la partición de test:\", acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rACyARKgnLn"
      },
      "source": [
        "metrics.plot_confusion_matrix(dt, X_test, y_test,cmap='binary')\n",
        "plt.title(\"Matriz de confusión obtenida para el clasificador Decision Tree\")\n",
        "plt.show()\n",
        "\n",
        "print(metrics.classification_report(y_test,y_pred))\n",
        "\n",
        "f1 = metrics.f1_score(y_test,y_pred,pos_label='immune')\n",
        "print(\"La medida F1 para el clasificador %s es %.4f\"%(dt.__class__.__name__,f1))\n",
        "\n",
        "y_probs = dt.predict_proba(X_test)\n",
        "auc = metrics.roc_auc_score(y_test, y_probs[:,1])\n",
        "print(\"La medida AUC para el clasificador %s es %.4f\"%(dt.__class__.__name__,auc))\n",
        "metrics.plot_roc_curve(dt, X_test, y_test)\n",
        "plt.title(\"Curva ROC obtenida para el clasificador Decision Tree\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZZ0xJXJnO59"
      },
      "source": [
        "Una de las grandes ventajas de los árboles de decisión es su buena interpretabilidad. Debido al uso de reglas sencillas en formato de árbol, es muy fácil determinar sus principales componentes, conocer las variables más útiles que representan el caso de estudio, o incluso dar una explicación directa para cada salida realizada por el clasificador, dado que será un único camino de la raíz al nodo hoja. \n",
        "\n",
        "En el siguiente bloque, se realiza una visualización del árbol generado, donde además se puede indicar explícitamente el nombre de las variables de entrada, así como las clases. Los colores (azul y naranja) indican las clases de salida mayoritarias en cada nodo, donde a mayor grado de color indica mejor separación de clases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G469ts8vnUCJ"
      },
      "source": [
        "#Bibliotecas necesarias para una mejor visualización\n",
        "from graphviz import Source\n",
        "\n",
        "#se pinta el árbol:\n",
        "tree_graph = tree.export_graphviz(dt, out_file=None, \n",
        "                                  feature_names=X.columns,\n",
        "                                  class_names=pd.unique(y[y.columns[0]]),\n",
        "                                  filled = True)\n",
        "graph = Source(tree_graph)\n",
        "graph\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKCkhKAnl6Mu"
      },
      "source": [
        "De igual modo que se realizó con el clasificador `LogisticRegressor` con un árbol de decisión resulta sencillo comprobar cuáles son las variables más importantes en el modelo. Por normal general, son aquéllas más cercanas al nodo raíz, pero existe también una propiedad de la clase `DecisionTreeClassifier` que nos permite acceder a esta información.\n",
        "\n",
        "Se puede observar que la variable seleccionada como la más importante de todas es justamente la que aparece en la raíz del árbol. Esto no es casualidad, debido a que es la que primero se selecciona para dividir las clases. \n",
        "\n",
        "Por último, puesto que el aprendizaje del modelo es diferente al del paradigma de clasificación lineal, el ranking de las variables más importantes también cambia. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfzKzPfhl6hy"
      },
      "source": [
        "#En primer lugar, se capturan los valores del ranking de importancia\n",
        "importancia = dt.feature_importances_\n",
        "#Se representan las 5 más importantes\n",
        "(pd.Series(importancia, index=X_train.columns).nlargest(5).plot(kind='barh'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRWE60Zp7tJu"
      },
      "source": [
        "Por último, se procede a representar el tipo de frontera de decisión obtenida por el paradigma de clasificación basado en árboles de decisión. \n",
        "\n",
        "En esta ocasión, se aprecia una división por bloques rectangulares, de acuerdo a cómo se distribuyen los ejemplos en cada nodo del árbol. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAlRHLAP7Hca"
      },
      "source": [
        "#Creamos y entrenamos el clasificador con los datos 2D\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf.fit(X_2D_train, y_2D_train)\n",
        "score = clf.score(X_2D_test,y_2D_test)\n",
        "\n",
        "fig = plt.figure(figsize=(12,9))\n",
        "fig = plot_decision_regions(clf=clf,X=X_2D,y=y_int.to_numpy().ravel(),\n",
        "                            X_highlight=X_2D_test, legend=2,\n",
        "                            scatter_kwargs=scatter_kwargs,\n",
        "                            contourf_kwargs=contourf_kwargs,\n",
        "                            scatter_highlight_kwargs=scatter_highlight_kwargs)\n",
        "plt.title('Frontera de decisión generada por el clasificador '+clf.__class__.__name__)\n",
        "plt.text(4 - .3, -3 + .3, ('Acc tst: %.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
        "handles, labels = fig.get_legend_handles_labels()\n",
        "fig.legend(handles, ['Clase MITF-low', 'Clase immune', 'Instancias test'], \n",
        "           framealpha=0.3, scatterpoints=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IOJpVqjLN70"
      },
      "source": [
        "### **4.3 Ventajas e inconvenientes de los árboles de decisión**\n",
        "\n",
        "Los árboles de decisión son una de las herramientas más potentes utilizadas en Machine Learning. Entre sus principales ventajas, se pueden destacar las siguientes:\n",
        "\n",
        "- Fácil de usar y eficiente. No contiene un excesivo número de parámetros, y éstos son sencillos de entender y configurar, para adaptarse mejor al problema. Además, es muy rápido en su ejecución, por lo que permite realizar continuas pruebas.\n",
        "- Las reglas generadas son fáciles de interpretar. Se ha comentado que una de las principales virtudes de los árboles de decisión es que son sistemas comprensibles por el usuario, puesto que utilizan reglas de cognición similares a las que aplicaría el experto. \n",
        "- Escalan mejor que otro tipo de técnicas. Si aumentamos el número de instancias o de variables, el rendimiento en tiempo de cómputo no se verá excesivamente afectado. \n",
        "- Puede manejar posibles datos ruidosos. Para ello, utiliza un mecanismo interno conocido como \"*poda*\" que permite reducir la profundidad del árbol de manera heurística en aras de una mejor generalización.\n",
        "\n",
        "El número de aspectos positivos de los árboles de decisión es bastante amplio; sin embargo, se deben tener en cuenta algunos detalles que pueden afectar a su uso:\n",
        "\n",
        "- No maneja directamente las variables de entrada de tipo numérico. Para el cálculo de las funciones de entropía o gini, las variables se deben discretizar previamente. Esto es transparente al usuario, pero debe ser tenido en cuenta. \n",
        "- Intenta dividir el dominio de la variable en regiones rectangulares. Este tipo de frontera de decisión puede no ser adecuada en algunas distribuciones de salida que sean lineales. \n",
        "- Tienen dificultades para lidiar con los valores perdidos. Se hace necesario imputar previamente estos valores.\n",
        "- Puede tener problemas de sobreaprendizaje. Si se aplica un factor muy alto de profundidad, será más dificil que generalice correctamente sobre las instancias de test.\n",
        "- No se detectan correlaciones entre las variables. Cada nodo de decisión se obtiene de forma independiente, sin tener en cuenta al resto. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjOTKGXWJOpS"
      },
      "source": [
        "## **REFERENCIAS BIBLIOGRÁFICAS**\n",
        "\n",
        "-\tHan, J., Kamber, M., Pei, J. (2011). Data Mining: Concepts and Techniques. San Francisco, CA, USA: Morgan Kaufmann Publishers. ISBN: 0123814790, 9780123814791\n",
        "-\tWitten, I. H., Frank, E., Hall, M. A., Pal, C. J. (2017). Data mining: practical machine learning tools and techniques. Amsterdam; London: Morgan Kaufmann. ISBN: 9780128042915 0128042915\n",
        "- Scikit-Learn: Supervised Learning https://scikit-learn.org/stable/supervised_learning.html (visitado el 25 de Junio de 2020).\n",
        "- Open Machine Learning Course: Topic 3. Classification, Decision Trees and k Nearest Neighbors https://mlcourse.ai/articles/topic3-dt-knn/ (visitado el 25 de Junio de 2020).\n",
        "\n",
        "\n",
        "### **Referencias adicionales**\n",
        "\n",
        "-\tAlpaydin, E. (2016). Machine Learning: The New AI. MIT Press. ISBN: 9780262529518\n",
        "- Towards Data Science: The Complete Guide to Classification in Python https://towardsdatascience.com/the-complete-guide-to-classification-in-python-b0e34c92e455 (visitado el 25 de Junio de 2020). \n",
        "- Towards Data Science: Python For Data Science — A Guide To Classification Machine Learning https://towardsdatascience.com/python-for-data-science-a-guide-to-classification-machine-learning-9ff51d237842 (visitado el 25 de Junio de 2020).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4RgRTPY7zF"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "MOOC Machine Learning y Big Data para la Bioinformática (1ª edición) \n",
        "\n",
        "http://abierta.ugr.es     \n",
        "\n",
        "![CC](https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-nd.png)\n",
        "</div>    "
      ]
    }
  ]
}